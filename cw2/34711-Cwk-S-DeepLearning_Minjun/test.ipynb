{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOP1wqW1CNpx93rSPpgnF+N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plNsHCM_BHF-","executionInfo":{"status":"ok","timestamp":1671246844447,"user_tz":0,"elapsed":2599,"user":{"displayName":"June Choi","userId":"06864138279760777107"}},"outputId":"a52b233f-5dae-4b0d-b8d7-49d1d1f6ebc1"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","import copy\n","import string\n","import time\n","import random\n","import numpy as np\n","import gensim\n","\n","from sklearn.cluster import KMeans\n","import nltk\n","nltk.download('words')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","\n","from gensim.models import Word2Vec\n","from nltk import pos_tag\n","from nltk.corpus import stopwords, wordnet, words\n","from nltk.util import ngrams\n","from nltk.stem import WordNetLemmatizer\n","from nltk.metrics.distance import jaccard_distance, edit_distance\n","from nltk.tokenize import TweetTokenizer, word_tokenize\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["file_path = \"drive/MyDrive/34711-Cwk-S-DeepLearning_Minjun/product_reviews/product_reviews\"\n","files = os.listdir(file_path)"],"metadata":{"id":"PCYlmOMiDmjd","executionInfo":{"status":"ok","timestamp":1671246582681,"user_tz":0,"elapsed":365,"user":{"displayName":"June Choi","userId":"06864138279760777107"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","\n","translational_table = str.maketrans(\n","    \"\",\n","    \"\",\n","    (string.punctuation) + \"§―•\\t←→\",\n",")\n","\n","correct_words = words.words()\n","stop_words = set(stopwords.words(\"english\"))\n","\n","wordnet_lemmatizer = WordNetLemmatizer()\n","# tweet_tokenizer = TweetTokenizer()\n","\n","\n","def pos_tagger(nltk_tag):\n","    \"\"\"\n","    Take a POS Tag, and return a wordnet equivalent tag to use for lemmatization\n","    \"\"\"\n","    if nltk_tag == None:\n","        return None\n","\n","    if nltk_tag.startswith(\"J\"):\n","        return wordnet.ADJ\n","    elif nltk_tag.startswith(\"V\"):\n","        return wordnet.VERB\n","    elif nltk_tag.startswith(\"N\"):\n","        return wordnet.NOUN\n","    elif nltk_tag.startswith(\"R\"):\n","        return wordnet.ADV\n","    else:\n","        return None\n","\n","\n","final_processed_reviews = []\n","processed_words = []\n","for filename in files:\n","    with open(file_path + \"/\" + filename, \"r\", encoding=\"utf-8-sig\") as file:\n","        raw_text = file.read()\n","\n","    raw_lines = [\n","        line for line in raw_text.lower().replace(\"/\", \" \").split(\"\\n\") if line != \"\"\n","    ]\n","\n","    reviews = []\n","    t_separate = []\n","    for i in range(len(raw_lines)):\n","        sentence = raw_lines[i].split(\"##\")\n","\n","        if len(sentence) < 2:\n","            reviews.append(copy.deepcopy(t_separate))\n","            t_separate = []\n","        else:\n","            t_separate.append(sentence[1])\n","\n","        if i == len(raw_lines) - 1:\n","            reviews.append(copy.deepcopy(t_separate))\n","\n","    reviews = [review for review in reviews if len(review) > 0]\n","\n","    for review in reviews:\n","        processed_lines = []\n","        for line in review:\n","            tokens_with_punctuations = word_tokenize(line)\n","\n","            ############ POS TAGGING AND LEMMATIZATION ############\n","            tokens_tags = pos_tag(tokens_with_punctuations)\n","\n","            # Preparing to lemmatize.\n","            # Changing from POS Tags to WordNet Tags\n","            wordnet_tags = [(x[0], pos_tagger(x[1])) for x in tokens_tags]\n","\n","            # Lemmatize with WordNet Lemmatizer\n","            lemmatized_tokens = [\n","                \"\" if tag is None else wordnet_lemmatizer.lemmatize(word, tag)\n","                for word, tag in wordnet_tags\n","            ]\n","            ####################################################################\n","\n","            # Removing stopwords\n","            lemmatized_uni_tokens_without_sw = [\n","                word for word in lemmatized_tokens if not word in stop_words\n","            ]\n","\n","            lemmatized_without_punct = [\n","                word.translate(translational_table)\n","                for word in lemmatized_uni_tokens_without_sw\n","                if word != \"\"\n","                and word != \"'s\"\n","                and word != \"'m\"\n","                and word != \"'re\"\n","                and word != \"'ve\"\n","                and word != \"n't\"\n","            ]\n","\n","            lemmatized_without_punct = [\n","                token\n","                for token in lemmatized_without_punct\n","                if token != \"\" and token.isnumeric() == False\n","            ]\n","\n","            processed_lines += lemmatized_without_punct\n","        processed_words += processed_lines\n","        final_processed_reviews += [processed_lines]\n","\n","dic = {}\n","for word in processed_words:\n","    if word in dic:\n","        dic[word] += 1\n","    else:\n","        dic[word] = 1\n","\n","# Sorting\n","sorted_words_counts = sorted(\n","    [[key, value] for key, value in dic.items()], key=lambda val: val[1], reverse=True\n",")\n","\n","# Acquiring top 50 words and reversed 50 words\n","top_50_words_counts = []\n","i = 0\n","while len(top_50_words_counts) < 50:\n","    if \"\".join(reversed(sorted_words_counts[i][0])) != sorted_words_counts[i][0]:\n","        top_50_words_counts.append(\n","            (sorted_words_counts[i][0], sorted_words_counts[i][1])\n","        )\n","    i += 1\n","\n","top_50_words = [item[0] for item in top_50_words_counts]\n","reverse_top_50_words = [\"\".join(reversed(word)) for word in top_50_words]\n","\n","# Dictionary for top 50 words\n","# Tracking counts with ->     word:count\n","top_50_words_counts = {item[0]: item[1] for item in top_50_words_counts}\n","\n","replace_list = []\n","for word in top_50_words:\n","    li = list(range(top_50_words_counts[word]))\n","    random.shuffle(li)\n","    replace_list.append(li[: top_50_words_counts[word] // 2])\n","print(len(replace_list[49]))\n","\n","replaced_processed_reviews = copy.deepcopy(final_processed_reviews)\n","\n","for target in range(len(top_50_words)):\n","    count = 0\n","    for r in range(len(replaced_processed_reviews)):\n","        for s in range(len(replaced_processed_reviews[r])):\n","            if replaced_processed_reviews[r][s] == top_50_words[target]:\n","                if count in replace_list[target]:\n","                    replaced_processed_reviews[r][s] = reverse_top_50_words[target]\n","                count += 1\n","\n","# print(replaced_processed_reviews)\n","\n","# print(\"***\" * 100)\n","# print(top_50_words)\n","\n","sg_model = Word2Vec(\n","    replaced_processed_reviews, min_count=1, vector_size=100, window=5, sg=1\n",")\n","# print(sg_model.wv[\"esu\"])\n","# print(sg_model.wv[\"use\"])\n","\n","feature_vecs = []\n","\n","one_hundred_words = top_50_words + reverse_top_50_words\n","\n","for word in one_hundred_words:\n","    feature_vec = sg_model.wv.get_vector(word)\n","    feature_vecs.append(feature_vec)\n","\n","percentages = []\n","for i in range(10):\n","    kmeans = KMeans(n_clusters=50, n_init='auto').fit(feature_vecs)\n","\n","    labels = kmeans.labels_\n","    print(labels)\n","\n","    # print(labels)\n","    p = 0\n","    for i in range(50):\n","        if labels[i] == labels[i + 50]:\n","            p += 1\n","    p = p / 50 * 100\n","    # print(\"correct pair\",p,\"percentage\")\n","    percentages.append(p)\n","percentages = np.array(percentages)\n","mean = np.mean(percentages)\n","print(mean)\n","std = np.std(percentages)\n","print(std)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"rrcx04yPDdIh","executionInfo":{"status":"error","timestamp":1671247155047,"user_tz":0,"elapsed":6720,"user":{"displayName":"June Choi","userId":"06864138279760777107"}},"outputId":"8635ca36-1b56-4240-f493-bf5a0b967a3e"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["47\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-a87a58f2ca52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m# print(top_50_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m sg_model = Word2Vec(\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mreplaced_processed_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m )\n","\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'vector_size'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"r8D2zK-3EuDl"},"execution_count":null,"outputs":[]}]}
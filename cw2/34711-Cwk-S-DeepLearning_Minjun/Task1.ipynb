{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Before running the codes **please use GPU** on collab so that results show up quicker.<br>\n",
        "Otherwise the results will come out very slow, which takes more than 10 hours."
      ],
      "metadata": {
        "id": "vIL2CGlDfFCY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKdMMQz1dg6S"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkjW3f2VdnFz",
        "outputId": "e59f1acb-df6d-4db8-db3d-34b916e680ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-XJdYXPduZ_"
      },
      "outputs": [],
      "source": [
        "!cd drive/MyDrive/Colab\\ Notebooks\n",
        "# set path to /content/.... as absolute path instead?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E2vsxjjdtV4",
        "outputId": "07389f21-7f2c-4a09-f477-1b6a35765600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zjhjVKMeTTQ",
        "outputId": "011f11aa-6f25-405a-c1c3-73dfd6474c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "# All imports\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from random import sample\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords, wordnet, words\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.metrics.distance import jaccard_distance, edit_distance\n",
        "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "\n",
        "\n",
        "# for use in removing stop words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# required for pos tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# required for lemmatization\n",
        "nltk.download('wordnet')\n",
        "# required for wordnet\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "print(torch.cuda.device_count())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxO2xw5YqFje",
        "outputId": "5f2d7b0c-7708-4dbf-ba81-17fa3cbcf976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Canon_PowerShot_SD500.txt', 'Canon_S100.txt', 'Diaper_Champ.txt', 'Hitachi_router.txt', 'Linksys_Router.txt', 'MicroMP3.txt', 'Nokia_6600.txt', 'ipod.txt', 'norton.txt']\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "stemmer = nltk.SnowballStemmer(\"english\", ignore_stopwords = False)\n",
        "# NB path AND corpus_after_token_reversal SHOULD BE CHANGED TO MATCH\n",
        "# THE CORPUS PATH ON THE SPECIFIC MACHINE\n",
        "\n",
        "# Folder path for the corpus\n",
        "corpus_path = \"drive/MyDrive/34711-Cwk-S-DeepLearning_Minjun/product_reviews\"\n",
        "\n",
        "# Folder path where the reverse token corpus should be stored\n",
        "corpus_after_token_reversal = r\"drive/MyDrive/34711-Cwk-S-DeepLearning_Minjun/processed_reviews\"\n",
        "file_pattern = r\".*\"\n",
        "original_corpus = nltk.corpus.PlaintextCorpusReader(corpus_path, file_pattern)\n",
        "print(original_corpus.fileids())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It should return:**\n",
        "\n",
        "['Canon_PowerShot_SD500.txt', 'Canon_S100.txt', 'Diaper_Champ.txt', 'Hitachi_router.txt', 'Linksys_Router.txt', 'MicroMP3.txt', 'Nokia_6600.txt', 'ipod.txt', 'norton.txt']\n",
        "\n",
        "<br>/////////////////////////////<br>\n",
        "readme.txt is not included in this corpus"
      ],
      "metadata": {
        "id": "kB_sbPLIrfu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ynnn63u2q7rP"
      },
      "outputs": [],
      "source": [
        "# Core utility function for document cleaning\n",
        "# Works recursively, split the text into sentences/review, then for each \n",
        "# sentence/review perform cleaning \n",
        "def process_doc(text, remove_punctuation, case_fold, stem,\n",
        "                remove_stopwords, remove_short_tokens, tokenize_by, manual_remove_list = [],\n",
        "                remove_nonalphabetical = False):\n",
        "\n",
        "  if (tokenize_by == \"sentence\"):\n",
        "    sentences = nltk.RegexpTokenizer(\"##\", gaps = True).tokenize(text)\n",
        "    sentences = [process_doc(sentence, remove_punctuation, case_fold, stem, \n",
        "                             remove_stopwords, remove_short_tokens, \"words\", manual_remove_list) \n",
        "                  for sentence in sentences]\n",
        "    return sentences\n",
        "  \n",
        "  if (tokenize_by == \"sentiments\"):\n",
        "    sentiments = nltk.RegexpTokenizer(\"\\[[\\-+][0-9]\\]\", gaps = True).tokenize(text)\n",
        "    sentiments = [process_doc(sentiment, remove_punctuation, case_fold, stem, \n",
        "                              remove_stopwords, remove_short_tokens, \"words\", manual_remove_list)\n",
        "                  for sentiment in sentiments]\n",
        "    return sentiments\n",
        "\n",
        "  # tokenizing in terms of reviews with [t]\n",
        "  if (tokenize_by == \"reviews\"):\n",
        "    reviews = nltk.RegexpTokenizer(\"\\[t\\]\", gaps = True).tokenize(text)\n",
        "    reviews = [process_doc(review, remove_punctuation, case_fold, stem, \n",
        "                              remove_stopwords, remove_short_tokens, \"words\", manual_remove_list)\n",
        "                for review in reviews]\n",
        "    return reviews\n",
        "  \n",
        "  # tokenizing in terms of words excluding punctuation\n",
        "  if (tokenize_by == \"words\"):\n",
        "    words = nltk.WordPunctTokenizer().tokenize(text)\n",
        "    if (remove_punctuation):\n",
        "      words = [w for w in words if w not in string.punctuation and w != \"...\" and w != \"]##\"]\n",
        "      # words = [w.strip(\"\") for w in words]\n",
        "\n",
        "    # preprocessing the tokenised words\n",
        "    if (case_fold):\n",
        "      words = [w.lower() for w in words]\n",
        "    if (remove_short_tokens):\n",
        "      words = [w for w in words if len(w) > 2]\n",
        "    if (stem):\n",
        "      words = [w if w in manual_remove_list else stemmer.stem(w) for w in words]\n",
        "    if (remove_stopwords):\n",
        "      words = [w for w in words if w not in stop_words and w != \"n't\" \n",
        "                and w != \"'s\"\n",
        "                and w != \"'m\"\n",
        "                and w != \"'re\"\n",
        "                and w != \"'ve\"]\n",
        "    if (remove_punctuation):\n",
        "      words = [w for w in words if w not in string.punctuation and w != \"...\" and w != \"]##\"]\n",
        "    if (remove_nonalphabetical):\n",
        "      words = [w for w in words if w.isalpha()]\n",
        "    return words\n",
        "\n",
        "def process_corpus(corpus, remove_punctuation:bool, case_fold:bool, stem:bool,\n",
        "                  remove_stopwords:bool, remove_short_tokens, tokenize_by:str, remove_nonalphabetical):\n",
        "  docs = [word for fileid in corpus.fileids() \n",
        "            for word in process_doc(corpus.raw(fileid), remove_punctuation, case_fold,\n",
        "                                    stem, remove_stopwords, remove_short_tokens, \n",
        "                                    tokenize_by, remove_nonalphabetical)\n",
        "         ]\n",
        "  return docs\n",
        "\n",
        "def most_frequent(words, n, do_print:bool):\n",
        "  freqDist = nltk.FreqDist(words)\n",
        "  most_common = freqDist.most_common(n)\n",
        "  if (do_print):\n",
        "    i = 1\n",
        "    for (w, count) in most_common:\n",
        "      print(i , w , count)\n",
        "      i += 1\n",
        "  return most_common\n",
        "\n",
        "# core function for generating corpus with reversed words\n",
        "# the corpus of reversed words is stored as files in the path specified by the variable:\n",
        "# corpus_after_token_reversal\n",
        "def generate_corpus_half_tokens_reversed(corpus, token_tuple_list, override_folder):\n",
        "  if not override_folder and os.path.exists(corpus_after_token_reversal):\n",
        "    return\n",
        "  if not os.path.exists(corpus_after_token_reversal):\n",
        "    os.mkdir(corpus_after_token_reversal)\n",
        "  # indecies_per_word = {word : list of 0s and 1s}\n",
        "  # if indecies_per_word[word][i] == 1\n",
        "  # the i-th occurrence of word needs to be reversed\n",
        "  indecies_per_word = {}\n",
        "  \n",
        "  # pointers keeps track of how many occurrences of each word we have met\n",
        "  pointers = {}\n",
        "\n",
        "  for (word, frequency) in token_tuple_list:\n",
        "    # construct an array with an equal number of 0-s and ones\n",
        "    indecies = np.ones(frequency)\n",
        "    indecies[:int(frequency/2)] = 0\n",
        "    \n",
        "    # shuffle it    \n",
        "    np.random.shuffle(indecies)\n",
        "    indecies_per_word[word] = indecies\n",
        "    pointers[word] = -1\n",
        "  fileids = corpus.fileids()\n",
        "  for fileid in fileids:\n",
        "    # tokenize the document\n",
        "    tokens = process_doc(corpus.raw(fileid), False, True, False, False, False, \"words\", False)\n",
        "    with_reversal = []\n",
        "    for token in tokens:\n",
        "      if (token in indecies_per_word):\n",
        "        # update the number of occurrences of the token\n",
        "        pointers[token] += 1\n",
        "       # determine whether to reverse the token\n",
        "        if (indecies_per_word[token][pointers[token]] == 1):\n",
        "         token = token[::-1]\n",
        "      with_reversal.append(token)\n",
        "    doc = \" \".join(with_reversal)\n",
        "    \n",
        "    f = open(os.path.join(corpus_after_token_reversal,fileid), \"w\")\n",
        "    f.write(doc)\n",
        "    f.close()\n",
        "  \n",
        "  # for (word, pointer) in pointers.items():\n",
        "  #   print (word, len(indecies_per_word[word]) - pointer - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l83s5VUXrFXa",
        "outputId": "48369b17-b852-4405-8dbb-92163f9139fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent 50 tokens in corpus after document cleaning and pre-processing\n",
            "1 use 353\n",
            "2 phone 320\n",
            "3 one 316\n",
            "4 ipod 314\n",
            "5 router 313\n",
            "6 camera 292\n",
            "7 player 269\n",
            "8 get 252\n",
            "9 battery 239\n",
            "10 like 195\n",
            "11 great 192\n",
            "12 quality 176\n",
            "13 good 176\n",
            "14 zen 174\n",
            "15 diaper 171\n",
            "16 product 166\n",
            "17 would 158\n",
            "18 also 156\n",
            "19 time 145\n",
            "20 software 145\n",
            "21 sound 144\n",
            "22 well 138\n",
            "23 really 136\n",
            "24 micro 136\n",
            "25 features 128\n",
            "26 computer 128\n",
            "27 easy 125\n",
            "28 even 123\n",
            "29 first 121\n",
            "30 used 120\n",
            "31 creative 118\n",
            "32 much 115\n",
            "33 better 114\n",
            "34 champ 113\n",
            "35 work 112\n",
            "36 want 107\n",
            "37 size 105\n",
            "38 music 105\n",
            "39 norton 104\n",
            "40 little 101\n",
            "41 need 100\n",
            "42 pictures 99\n",
            "43 works 99\n",
            "44 still 97\n",
            "45 buy 96\n",
            "46 problem 96\n",
            "47 mp3 96\n",
            "48 price 91\n",
            "49 life 91\n",
            "50 using 91\n"
          ]
        }
      ],
      "source": [
        "#### Step 1 ####\n",
        "\n",
        "print(\"Most frequent 50 tokens in corpus after document cleaning and pre-processing\")\n",
        "\n",
        "# cleaning and pre-processing\n",
        "# then save the processed corpus in different directory\n",
        "processed_corpus = process_corpus(original_corpus, True, True, False, True, True, \"words\", True)\n",
        "\n",
        "# Choosing top 50 frequently occured words\n",
        "top_50 = most_frequent(processed_corpus, 50, True)\n",
        "cluster_words = set()\n",
        "for (word, freq) in top_50:\n",
        "  cluster_words.add(word)\n",
        "  cluster_words.add(word[::-1])\n",
        "# generate_corpus_half_tokens_reversed(original_corpus, most_frequent_tokens, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most frequent 50 tokens in corpus after document cleaning and pre-processing<br>\n",
        "1 use 353<br>\n",
        "2 phone 320<br>\n",
        "3 one 316<br>\n",
        "4 ipod 314<br>\n",
        "5 router 313<br>\n",
        "6 camera 292<br>\n",
        "7 player 269<br>\n",
        "8 get 252<br>\n",
        "9 battery 239<br>\n",
        "10 like 195<br>\n",
        "11 great 192<br>\n",
        "12 quality 176<br>\n",
        "13 good 176<br>\n",
        "14 zen 174<br>\n",
        "15 diaper 171<br>\n",
        "16 product 166<br>\n",
        "17 would 158<br>\n",
        "18 also 156<br>\n",
        "19 time 145<br>\n",
        "20 software 145<br>\n",
        "21 sound 144<br>\n",
        "22 well 138<br>\n",
        "23 really 136<br>\n",
        "24 micro 136<br>\n",
        "25 features 128<br>\n",
        "26 computer 128<br>\n",
        "27 easy 125<br>\n",
        "28 even 123<br>\n",
        "29 first 121<br>\n",
        "30 used 120<br>\n",
        "31 creative 118<br>\n",
        "32 much 115<br>\n",
        "33 better 114<br>\n",
        "34 champ 113<br>\n",
        "35 work 112<br>\n",
        "36 want 107<br>\n",
        "37 size 105<br>\n",
        "38 music 105<br>\n",
        "39 norton 104<br>\n",
        "40 little 101<br>\n",
        "41 need 100<br>\n",
        "42 pictures 99<br>\n",
        "43 works 99<br>\n",
        "44 still 97<br>\n",
        "45 buy 96<br>\n",
        "46 problem 96<br>\n",
        "47 mp3 96<br>\n",
        "48 price 91<br>\n",
        "49 life 91<br>\n",
        "50 using 91<br>"
      ],
      "metadata": {
        "id": "8r6K-Ej6-vl5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38En6RiNsg24"
      },
      "outputs": [],
      "source": [
        "# cleanning and pre-processing\n",
        "\n",
        "def clean_all_sentences(corpus_path, stemming, stopwords_removal, manual_remove_list = [\"§\", \"―\",\"•\",\"\\t\",\"←\",\"→\"]):\n",
        "\n",
        "  corpus = nltk.corpus.PlaintextCorpusReader(corpus_path, file_pattern)\n",
        "  out = []\n",
        "  for fileid in corpus.fileids():\n",
        "    sentences = process_doc(corpus.raw(fileid), True, True, stemming, stopwords_removal, True, \"sentence\", manual_remove_list, True)\n",
        "    out.extend(sentences)\n",
        "  return out\n",
        "\n",
        "# generating word to index and index to word dictonary\n",
        "def generate_w_to_idx_and_idx_to_w(corpus):\n",
        "  word_to_idx = {}\n",
        "  idx_to_word = {}\n",
        "  i = 0\n",
        "  for sentence in corpus:\n",
        "    for word in sentence:\n",
        "      if (word not in word_to_idx):\n",
        "        word_to_idx[word] = i\n",
        "        idx_to_word[i] = word\n",
        "        i += 1\n",
        "\n",
        "  return (word_to_idx, idx_to_word)\n",
        "\n",
        "\n",
        "def get_context_window_tuples(word_to_idx, sentences, window, key_words):\n",
        "  tuples = []\n",
        "  for sentence in sentences:\n",
        "    for i in range(window, len(sentence) - window):\n",
        "        context = []\n",
        "        middle_word = word_to_idx[sentence[i]]\n",
        "        for j in range (i - window, i + window + 1):\n",
        "          if i != j:\n",
        "            context.append(word_to_idx[sentence[j]])\n",
        "        tuples.append((context, word_to_idx[sentence[i]]))\n",
        "\n",
        "  return tuples\n",
        "\n",
        "\n",
        "# def merge_term_to_term_dicts(term_to_term_dicts):\n",
        "#   term_to_term = defaultdict(lambda: defaultdict(int))\n",
        "#   for term_to_term_dict in term_to_term_dicts:\n",
        "#     for (key_word, freqs) in term_to_term_dict.items():\n",
        "#       for (value_word, freq) in freqs.items():\n",
        "#         term_to_term[key_word][value_word] += freq\n",
        "#   return term_to_term\n",
        "\n",
        "# Used Skip-Grams Model\n",
        "def get_skipgrams(sentences, word_to_idx, window, neg_sample_count):\n",
        "  word = []\n",
        "  context = []\n",
        "  y = []\n",
        "  for sentence in sentences:\n",
        "    for i in range(len(sentence)):\n",
        "      cont = [word_to_idx[sentence[idx]] for idx in range(max(0, i - window), min(len(sentence), i + window + 1)) if idx != i]\n",
        "      blacklist = set(cont)\n",
        "      word.extend([word_to_idx[sentence[i]]] * (len(cont)))\n",
        "      context.extend(cont)\n",
        "\n",
        "  return(word, context)\n",
        "\n",
        "\n",
        "def get_batches(words, contexts, batch_size):\n",
        "  shuffled_idxs = sample(range(0, len(words)), len(words))\n",
        "  batches = []\n",
        "\n",
        "  batch_word, batch_context = [], []\n",
        "  for i in range(len(words)):\n",
        "    idx = shuffled_idxs[i]\n",
        "    batch_word.append(words[idx])\n",
        "    batch_context.append(contexts[idx])\n",
        "    if (i + 1) % batch_size == 0 or i + 1 == len(words):\n",
        "      batches.append((\n",
        "        torch.from_numpy(np.array(batch_word)),\n",
        "        torch.from_numpy(np.array(batch_context))\n",
        "      ))\n",
        "      batch_word, batch_context = [], []\n",
        "\n",
        "  return batches\n",
        "\n",
        "\n",
        "def get_x_tensors(x_y_tuples):\n",
        "  tensors = []\n",
        "  for tuple in x_y_tuples:\n",
        "    tensors.append(torch.tensor(tuple[0], dtype=torch.long))\n",
        "\n",
        "  return tensors\n",
        "\n",
        "\n",
        "def get_y_tensors(tuples, num_classes):\n",
        "  tensors = []\n",
        "  for tuple in tuples:\n",
        "    tensors.append(torch.tensor([tuple[1]]))\n",
        "  return tensors\n",
        "# def get_context_words_count(term_to_term_dict):\n",
        "#   counts = defaultdict(int)\n",
        "#   for (key, freqs) in term_to_term_dict.items():\n",
        "#     for (context, freq) in freqs.items():\n",
        "#       counts[context] += freq\n",
        "  \n",
        "#   return counts\n",
        "\n",
        "\n",
        "# def get_coocurrences_alpha(context_counts, alpha=0.75):\n",
        "#   sum = 0\n",
        "#   for context_count in context_counts.values():\n",
        "#     sum += pow(context_count, alpha)\n",
        "#   return sum\n",
        "\n",
        "# def generate_context_word_mapping(term_to_term_dict):\n",
        "#   i = 0\n",
        "#   mapping = {}\n",
        "#   for (key, freqs) in term_to_term_dict.items():\n",
        "#     for (context, freq) in freqs.items():\n",
        "#       if context not in mapping:\n",
        "#         mapping[context] = i\n",
        "#         i += 1\n",
        "#   return mapping\n",
        "\n",
        "# def calculate_all_coocurrences(count_dict):\n",
        "#   sum = 0\n",
        "#   for val in count_dict.values():\n",
        "#     sum += val\n",
        "#   return sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OjoFxIHswPc"
      },
      "outputs": [],
      "source": [
        "class Word2Vec_Skipgram(nn.Module):\n",
        "  def __init__(self, embedding_size, vocab_size) -> None:\n",
        "        super(Word2Vec_Skipgram, self).__init__()\n",
        "        # matches each word to a vector of values\n",
        "        self.embedding_words = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
        "        # convert the linear layer output to a probability distribution\n",
        "        self.log_softmax = nn.LogSoftmax(dim = 1)\n",
        "    \n",
        "  def forward(self, words):\n",
        "      words_emb = self.embedding_words(words)\n",
        "      scores = self.linear(words_emb)\n",
        "      log_probs = self.log_softmax(scores)\n",
        "      return log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7zJ1zkNsz8e"
      },
      "outputs": [],
      "source": [
        "def train_skipgram_model(model, epochs, batch_size, learning_rate, print_feature_vector, words, contexts):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  loss_function = nn.NLLLoss()\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for inputs,targets in get_batches(words=words, contexts=contexts, batch_size=300):\n",
        "      optimizer.zero_grad()\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      y_hat = model(inputs)\n",
        "      loss = loss_function(y_hat, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss\n",
        "    if (print_feature_vector):\n",
        "      print(epoch, total_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXKetaehs2TW"
      },
      "outputs": [],
      "source": [
        "def clustering_get_accuracy(n_clusters, keys, matrix, cluster_method, flag_empty_clusters, print_cluster=False):   \n",
        "  if (cluster_method == \"kmeans\"):                \n",
        "    cluster_algo = KMeans(n_clusters)\n",
        "  elif (cluster_method == \"agglomerative\"):\n",
        "      cluster_algo = AgglomerativeClustering(\n",
        "        n_clusters=n_clusters\n",
        "      )\n",
        "  elif (cluster_method == \"agglomerative_complete\"):\n",
        "    cluster_algo = AgglomerativeClustering(\n",
        "      n_clusters=n_clusters,\n",
        "      linkage=\"complete\",\n",
        "      affinity=\"cosine\"\n",
        "    )\n",
        "  cluster_algo.fit(matrix)\n",
        "  clusters = []\n",
        "  for i in range(50):\n",
        "    clusters.append(set())\n",
        "  i = 0\n",
        "  for label in cluster_algo.labels_:\n",
        "    clusters[label].add(keys[i])\n",
        "    i += 1\n",
        "  correct = 0\n",
        "  for cluster in clusters:\n",
        "    if (flag_empty_clusters and len(cluster) == 0):\n",
        "      print(\"EMPTY CLUSTER DETECTED\")\n",
        "    if (print_cluster): \n",
        "      print(cluster)\n",
        "    for word in cluster:\n",
        "      if word[::-1] in cluster:\n",
        "        correct += 1\n",
        "  return correct / len(keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LggQ5NmwtWAS"
      },
      "outputs": [],
      "source": [
        "def get_target_words_embeddings(target_words, embedding_matrix, word_to_idx):\n",
        "  keys = []\n",
        "  matrix = []\n",
        "  for key in target_words:\n",
        "    idx = word_to_idx[key]\n",
        "    matrix.append(embedding_matrix[idx])\n",
        "    keys.append(key)\n",
        "  return (keys, matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cswZ6riwsnVo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment(iterations, corpus_path, training_epochs, embedding_dims, window_size, cluster_method, learning_rate, stemming, stopwords_removal, print_feature_vector = False):\n",
        "  accuracy = np.zeros(iterations)\n",
        "\n",
        "  # Get the 50 most common words for the experiment\n",
        "  processed_corpus = process_corpus(original_corpus, True, True, False, True, True, \"words\", True)\n",
        "  top_50 = most_frequent(processed_corpus, 50, False)\n",
        "  cluster_words = set()\n",
        "\n",
        "  for (word, freq) in top_50:\n",
        "    cluster_words.add(word)\n",
        "    cluster_words.add(word[::-1])\n",
        "  \n",
        "  for iteration in range(iterations):\n",
        "    # reverse half of instances of most common words at random\n",
        "    generate_corpus_half_tokens_reversed(original_corpus, top_50, True)\n",
        "    # clean sentences\n",
        "    sentences = clean_all_sentences(corpus_after_token_reversal, stemming, stopwords_removal, cluster_words)\n",
        "\n",
        "    # set up data\n",
        "    (word_to_idx, idx_to_word) = generate_w_to_idx_and_idx_to_w(sentences)\n",
        "    vocab_size = len(word_to_idx)\n",
        "    tuples = get_context_window_tuples(word_to_idx, sentences, window_size, cluster_words)\n",
        "    (words, contexts) = get_skipgrams(sentences, word_to_idx, window_size, 10)\n",
        "\n",
        "    # train model\n",
        "    skipgrams_model = Word2Vec_Skipgram(embedding_dims, vocab_size=vocab_size).to(device)\n",
        "    train_skipgram_model(skipgrams_model, training_epochs, 500, learning_rate, False, words, contexts)\n",
        "\n",
        "    # get embeddings\n",
        "    embedding_matrix = skipgrams_model.embedding_words.weight.detach().cpu().numpy()\n",
        "    (target_words, embeddings) = get_target_words_embeddings(cluster_words, embedding_matrix, word_to_idx)\n",
        "\n",
        "    # perform clustering\n",
        "    accuracy[iteration] = clustering_get_accuracy(50, target_words, embeddings, cluster_method, True, print_feature_vector)\n",
        "    if (print_feature_vector):\n",
        "      print(\"Iteration\", iteration + 1, \"Accuracy:\", accuracy[iteration])\n",
        "  return (\"Average accuracy:\", np.mean(accuracy), \"Standard diviation:\", np.std(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJmIvP3u3Fdp"
      },
      "outputs": [],
      "source": [
        "print(\"Performance with window size 1:\",\n",
        "      run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=20, embedding_dims=100, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with window size 2:\",\n",
        "      run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=20, embedding_dims=100, window_size=2, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with window size 3:\",\n",
        "      run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=20, embedding_dims=100, window_size=3, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with window size 4:\",\n",
        "      run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=20, embedding_dims=100, window_size=4, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with window size 5:\",\n",
        "      run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=20, embedding_dims=100, window_size=5, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with window size 10:\",\n",
        "      run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=20, embedding_dims=100, window_size=10, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results took hours to come out, so I am recording them in this text.<br> \n",
        "Performance with window size 1: ('Average accuracy:', 0.8720000000000001, 'Standard diviation:', 0.029933259094191558)<br>\n",
        "Performance with window size 2: ('Average accuracy:', 0.8560000000000001, 'Standard diviation:', 0.008000000000000007)<br>\n",
        "Performance with window size 3: ('Average accuracy:', 0.8400000000000001, 'Standard diviation:', 0.055136195008360894)<br>\n",
        "Performance with window size 4: ('Average accuracy:', 0.788, 'Standard diviation:', 0.06013318551349164)<br>\n",
        "Performance with window size 5: ('Average accuracy:', 0.716, 'Standard diviation:', 0.06499230723708765)<br>\n",
        "Performance with window size 10: ('Average accuracy:', 0.5680000000000001, 'Standard diviation:', 0.015999999999999966)"
      ],
      "metadata": {
        "id": "vKTM9JwHaPgR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THNSRmxisrlP"
      },
      "outputs": [],
      "source": [
        "print(\"Performance with word embedding dimension length of 50:\",\n",
        "    run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=40, embedding_dims=50, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with embedding dimension length of 100:\",\n",
        "    run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=40, embedding_dims=100, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with embedding dimension length of 150:\",\n",
        "    run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=40, embedding_dims=150, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with embedding dimension length of 200:\",\n",
        "    run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=40, embedding_dims=200, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "print(\"Performance with embedding dimension length of 300:\",\n",
        "    run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=40, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with embedding dimension length of 400:\",\n",
        "    run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=40, embedding_dims=400, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance with word embedding dimension length of 50: ('Average accuracy:', 0.8200000000000001, 'Standard diviation:', 0.07266360849833982)<br>\n",
        "Performance with embedding dimension length of 100: ('Average accuracy:', 0.884, 'Standard diviation:', 0.023323807579381222)<br>\n",
        "Performance with embedding dimension length of 150: ('Average accuracy:', 0.8960000000000001, 'Standard diviation:', 0.01959591794226544)<br>\n",
        "Performance with embedding dimension length of 200: ('Average accuracy:', 0.868, 'Standard diviation:', 0.04833218389437827)<br>\n",
        "Performance with embedding dimension length of 300: ('Average accuracy:', 0.86, 'Standard diviation:', 0.03794733192202055)<br>\n",
        "Performance with embedding dimension length of 400: ('Average accuracy:', 0.8959999999999999, 'Standard diviation:', 0.057131427428342804)<br>"
      ],
      "metadata": {
        "id": "ln-o5qsScNMT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waNpE9MN3Ltm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d34541d-89cf-4d7e-8fd6-33d4bda546ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance after removing stopwords and stemming: ('Average accuracy:', 0.8160000000000001, 'Standard diviation:', 0.04630334761116089)\n"
          ]
        }
      ],
      "source": [
        "print(\"Performance after Stemming: \",\n",
        "       run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=True, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance after removing stopwords:\",\n",
        "      run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=True, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance after stemming and removing stopwords:\",\n",
        "      run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=True, stopwords_removal=True, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance without both pre-processing: \",\n",
        "       run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance after Stemming:  ('Average accuracy:', 0.9039999999999999, 'Standard diviation:', 0.03878143885933062)<br>\n",
        "Performance after removing stopwords: ('Average accuracy:', 0.7959999999999999, 'Standard diviation:', 0.032)<br>\n",
        "Performance after stemming and removing stopwords: ('Average accuracy:', 0.8160000000000001, 'Standard diviation:', 0.04630334761116089)<br>\n",
        "Performance without both pre-processing:  ('Average accuracy:', 0.9400000000000001, 'Standard diviation:', 0.025298221281347004)<br>"
      ],
      "metadata": {
        "id": "3UKPQXEOjnGo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtBpIxZY4R2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d85aac7-a49c-4fc0-a89c-ba3bfa971025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance with 10 Epochs:  ('Average accuracy:', 0.892, 'Standard diviation:', 0.02400000000000002)\n",
            "Performance with 20 Epochs:  ('Average accuracy:', 0.884, 'Standard diviation:', 0.023323807579381222)\n",
            "Performance with 30 Epochs:  ('Average accuracy:', 0.876, 'Standard diviation:', 0.014966629547095779)\n",
            "Performance with 40 Epochs:  ('Average accuracy:', 0.8800000000000001, 'Standard diviation:', 0.04898979485566354)\n"
          ]
        }
      ],
      "source": [
        "print(\"Performance with 10 Epochs: \",\n",
        "       run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=10, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with 20 Epochs: \",\n",
        "       run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with 30 Epochs: \",\n",
        "       run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=30, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))\n",
        "\n",
        "print(\"Performance with 40 Epochs: \",\n",
        "       run_experiment(iterations=5, corpus_path=corpus_path, training_epochs=40, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance with 10 Epochs:  ('Average accuracy:', 0.892, 'Standard diviation:', 0.02400000000000002)<br>\n",
        "Performance with 20 Epochs:  ('Average accuracy:', 0.884, 'Standard diviation:', 0.023323807579381222)<br>\n",
        "Performance with 30 Epochs:  ('Average accuracy:', 0.876, 'Standard diviation:', 0.014966629547095779)<br>\n",
        "Performance with 40 Epochs:  ('Average accuracy:', 0.8800000000000001, 'Standard diviation:', 0.04898979485566354)<br>"
      ],
      "metadata": {
        "id": "HKtto4flvPaa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHX8P5jg4TFi"
      },
      "outputs": [],
      "source": [
        " ### From the experiments, 20 epoches and dimension length of 300 with window_size 1 had the \n",
        " print(\"Best performance: \",\n",
        "       run_experiment(iterations = 5, corpus_path=corpus_path, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               print_feature_vector=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'good', 'doog', 'great', 'taerg'}\n",
        "{'software', 'erawtfos'}\n",
        "{'gnisu', 'using'}\n",
        "{'size', 'ezis'}\n",
        "{'tcudorp', 'product'}\n",
        "{'serutcip', 'pictures'}\n",
        "{'much', 'hcum'}\n",
        "{'krow', 'work'}\n",
        "{'ekil', 'like'}\n",
        "{'even', 'neve'}\n",
        "{'tsrif', 'first'}\n",
        "{'desu', 'used'}\n",
        "{'orcim', 'micro', 'creative', 'evitaerc'}\n",
        "{'champ', 'pmahc'}\n",
        "{'esu', 'use'}\n",
        "{'yub', 'buy'}\n",
        "{'retteb', 'better'}\n",
        "{'player', 'reyalp'}\n",
        "{'dopi', 'ipod'}\n",
        "{'want', 'tnaw'}\n",
        "{'easy', 'ysae'}\n",
        "{'llew', 'well'}\n",
        "{'also', 'osla'}\n",
        "{'serutaef', 'features'}\n",
        "{'llits'}\n",
        "{'would', 'dluow'}\n",
        "{'retuor', 'router'}\n",
        "{'norton', 'notron'}\n",
        "{'time', 'emit'}\n",
        "{'cisum', 'music'}\n",
        "{'yrettab', 'battery'}\n",
        "{'sound', 'dnuos'}\n",
        "{'aremac', 'camera'}\n",
        "{'computer'}\n",
        "{'teg', 'get'}\n",
        "{'repaid', 'diaper'}\n",
        "{'eno', 'one'}\n",
        "{'little', 'elttil'}\n",
        "{'price', 'ecirp'}\n",
        "{'ytilauq', 'quality'}\n",
        "{'problem', 'melborp'}\n",
        "{'3pm', 'mp3'}\n",
        "{'enohp', 'phone'}\n",
        "{'life', 'efil'}\n",
        "{'need', 'deen'}\n",
        "{'really', 'yllaer'}\n",
        "{'works', 'skrow'}\n",
        "{'retupmoc'}\n",
        "{'zen', 'nez'}\n",
        "{'still'}<br>\n",
        "Iteration 1 Accuracy: 0.96<br>\n",
        "{'champ', 'repaid', 'diaper', 'pmahc'}\n",
        "{'yub', 'get', 'buy'}\n",
        "{'aremac', 'camera', 'product'}\n",
        "{'computer', 'retupmoc'}\n",
        "{'serutcip', 'pictures'}\n",
        "{'need', 'deen'}\n",
        "{'gnisu', 'using'}\n",
        "{'serutaef', 'features'}\n",
        "{'even', 'neve'}\n",
        "{'time', 'emit'}\n",
        "{'krow', 'work'}\n",
        "{'really', 'yllaer'}\n",
        "{'much', 'hcum'}\n",
        "{'easy', 'ysae'}\n",
        "{'doog', 'great'}\n",
        "{'ekil', 'like'}\n",
        "{'little', 'elttil'}\n",
        "{'llits', 'still'}\n",
        "{'retteb', 'better'}\n",
        "{'problem', 'melborp'}\n",
        "{'want', 'tnaw'}\n",
        "{'software', 'erawtfos'}\n",
        "{'size', 'ecirp'}\n",
        "{'would', 'dluow'}\n",
        "{'yrettab', 'battery'}\n",
        "{'price'}\n",
        "{'desu', 'used'}\n",
        "{'teg'}\n",
        "{'ytilauq', 'quality'}\n",
        "{'llew', 'well'}\n",
        "{'sound', 'dnuos'}\n",
        "{'dopi', 'ipod'}\n",
        "{'life', 'efil'}\n",
        "{'norton', 'notron'}\n",
        "{'tsrif', 'first'}\n",
        "{'player', 'reyalp'}\n",
        "{'also', 'osla'}\n",
        "{'cisum', 'music'}\n",
        "{'retuor', 'router'}\n",
        "{'3pm', 'mp3'}\n",
        "{'eno', 'one'}\n",
        "{'enohp', 'phone'}\n",
        "{'creative', 'evitaerc'}\n",
        "{'works', 'skrow'}\n",
        "{'orcim', 'micro'}\n",
        "{'esu', 'use'}\n",
        "{'tcudorp'}\n",
        "{'good', 'taerg'}\n",
        "{'zen', 'nez'}\n",
        "{'ezis'}<br>\n",
        "Iteration 2 Accuracy: 0.88<br>\n",
        "{'easy', 'ysae', 'well', 'llew'}\n",
        "{'want', 'need', 'deen', 'tnaw'}\n",
        "{'orcim', 'micro', 'creative', 'evitaerc'}\n",
        "{'software', 'erawtfos'}\n",
        "{'teg', 'get'}\n",
        "{'good', 'doog', 'great', 'taerg'}\n",
        "{'serutcip', 'pictures'}\n",
        "{'gnisu', 'using'}\n",
        "{'size', 'ezis'}\n",
        "{'would', 'dluow'}\n",
        "{'much', 'hcum'}\n",
        "{'ekil', 'like'}\n",
        "{'time', 'emit'}\n",
        "{'esu', 'use'}\n",
        "{'serutaef', 'features'}\n",
        "{'little', 'elttil'}\n",
        "{'even', 'neve'}\n",
        "{'really', 'yllaer'}\n",
        "{'price', 'ecirp'}\n",
        "{'aremac', 'camera'}\n",
        "{'sound', 'dnuos'}\n",
        "{'champ', 'pmahc'}\n",
        "{'krow', 'work'}\n",
        "{'eno', 'one'}\n",
        "{'tsrif', 'first'}\n",
        "{'player', 'reyalp'}\n",
        "{'dopi', 'ipod'}\n",
        "{'3pm', 'mp3'}\n",
        "{'life', 'efil'}\n",
        "{'norton', 'notron'}\n",
        "{'desu', 'used'}\n",
        "{'problem', 'melborp'}\n",
        "{'llits'}\n",
        "{'enohp', 'phone'}\n",
        "{'retteb', 'better'}\n",
        "{'product'}\n",
        "{'computer'}\n",
        "{'also'}\n",
        "{'yub', 'buy'}\n",
        "{'yrettab', 'battery'}\n",
        "{'repaid', 'diaper'}\n",
        "{'still'}\n",
        "{'retuor', 'router'}\n",
        "{'zen', 'nez'}\n",
        "{'cisum', 'music'}\n",
        "{'tcudorp'}\n",
        "{'works', 'skrow'}\n",
        "{'retupmoc'}\n",
        "{'osla'}\n",
        "{'ytilauq', 'quality'}<br>\n",
        "Iteration 3 Accuracy: 0.92<br>\n",
        "{'also', 'osla'}\n",
        "{'want', 'need', 'deen', 'tnaw'}\n",
        "{'llits', 'still'}\n",
        "{'orcim', 'micro', 'creative', 'evitaerc'}\n",
        "{'serutaef', 'features'}\n",
        "{'krow', 'work'}\n",
        "{'gnisu', 'using'}\n",
        "{'computer', 'retupmoc'}\n",
        "{'even', 'neve'}\n",
        "{'much', 'hcum'}\n",
        "{'teg', 'get'}\n",
        "{'retteb', 'better'}\n",
        "{'ekil', 'like'}\n",
        "{'yub', 'buy'}\n",
        "{'tsrif', 'first'}\n",
        "{'esu', 'use'}\n",
        "{'desu', 'used'}\n",
        "{'little', 'elttil'}\n",
        "{'player', 'reyalp'}\n",
        "{'size', 'ezis'}\n",
        "{'sound', 'dnuos'}\n",
        "{'llew', 'well'}\n",
        "{'really', 'yllaer'}\n",
        "{'good', 'doog'}\n",
        "{'software', 'erawtfos'}\n",
        "{'would', 'dluow'}\n",
        "{'retuor', 'router'}\n",
        "{'pictures'}\n",
        "{'eno', 'one'}\n",
        "{'champ', 'pmahc'}\n",
        "{'yrettab', 'battery'}\n",
        "{'cisum', 'music'}\n",
        "{'dopi', 'ipod'}\n",
        "{'enohp', 'phone'}\n",
        "{'ytilauq', 'quality'}\n",
        "{'time', 'emit'}\n",
        "{'works', 'skrow'}\n",
        "{'price', 'ecirp'}\n",
        "{'life', 'efil'}\n",
        "{'repaid', 'diaper'}\n",
        "{'easy', 'ysae'}\n",
        "{'zen', 'nez'}\n",
        "{'tcudorp'}\n",
        "{'problem', 'melborp'}\n",
        "{'product'}\n",
        "{'aremac', 'camera'}\n",
        "{'norton', 'notron'}\n",
        "{'3pm', 'mp3'}\n",
        "{'great', 'taerg'}\n",
        "{'serutcip'}<br>\n",
        "Iteration 4 Accuracy: 0.96<br>\n",
        "{'want', 'need', 'deen', 'tnaw'}\n",
        "{'champ', 'repaid', 'diaper', 'pmahc'}\n",
        "{'price', 'ecirp'}\n",
        "{'good', 'doog', 'great', 'taerg'}\n",
        "{'also', 'serutcip'}\n",
        "{'ekil', 'like'}\n",
        "{'time', 'emit'}\n",
        "{'dopi', 'ipod'}\n",
        "{'easy', 'ysae', 'well'}\n",
        "{'little', 'elttil'}\n",
        "{'orcim', 'micro', 'creative', 'evitaerc'}\n",
        "{'much', 'hcum'}\n",
        "{'serutaef', 'features'}\n",
        "{'software', 'erawtfos'}\n",
        "{'dnuos', 'pictures'}\n",
        "{'computer', 'retupmoc'}\n",
        "{'would', 'dluow'}\n",
        "{'desu', 'used'}\n",
        "{'teg', 'get'}\n",
        "{'esu', 'use'}\n",
        "{'life', 'efil'}\n",
        "{'really', 'yllaer'}\n",
        "{'tsrif', 'first'}\n",
        "{'norton', 'notron'}\n",
        "{'llits'}\n",
        "{'retteb', 'better'}\n",
        "{'3pm', 'mp3'}\n",
        "{'player', 'reyalp'}\n",
        "{'works', 'skrow'}\n",
        "{'aremac', 'camera'}\n",
        "{'eno', 'one'}\n",
        "{'yub', 'buy'}\n",
        "{'enohp', 'phone'}\n",
        "{'product'}\n",
        "{'size', 'ezis'}\n",
        "{'llew'}\n",
        "{'ytilauq', 'quality'}\n",
        "{'even', 'neve'}\n",
        "{'still'}\n",
        "{'yrettab', 'battery'}\n",
        "{'retuor', 'router'}\n",
        "{'gnisu'}\n",
        "{'osla'}\n",
        "{'problem', 'melborp'}\n",
        "{'using'}\n",
        "{'krow', 'work'}\n",
        "{'zen', 'nez'}\n",
        "{'sound'}\n",
        "{'cisum', 'music'}\n",
        "{'tcudorp'}<br>\n",
        "Iteration 5 Accuracy: 0.86<br>\n",
        "Best performance:  ('Average accuracy:', 0.916, 'Standard diviation:', 0.04079215610874227)"
      ],
      "metadata": {
        "id": "P5oMnzP6nzIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ### From the experiments, 20 epoches and dimension length of 300 with window_size 1 had the \n",
        " print(\"Best performance with preprocessing: \",\n",
        "       run_experiment(iterations = 5, corpus_path=corpus_path, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=True, stopwords_removal=True, \n",
        "               print_feature_vector=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C9LHV6N8LV4",
        "outputId": "49ea3d2d-0c83-462d-9262-0b85577691a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'elttil', 'really'}\n",
            "{'micro', 'orcim', 'evitaerc', 'creative'}\n",
            "{'llits', 'price'}\n",
            "{'use', 'esu', 'using'}\n",
            "{'erawtfos', 'software'}\n",
            "{'pmahc', 'champ', 'diaper', 'repaid'}\n",
            "{'retupmoc', 'computer'}\n",
            "{'yllaer', 'still'}\n",
            "{'player', 'reyalp'}\n",
            "{'tnaw', 'ekil'}\n",
            "{'like', 'used'}\n",
            "{'get', 'yub', 'buy'}\n",
            "{'skrow', 'works', 'krow', 'work'}\n",
            "{'problem', 'melborp'}\n",
            "{'enohp', 'phone'}\n",
            "{'llew', 'well'}\n",
            "{'features', 'serutaef'}\n",
            "{'sound', 'dnuos'}\n",
            "{'first', 'tsrif'}\n",
            "{'retteb', 'better'}\n",
            "{'ipod', 'dopi'}\n",
            "{'camera', 'aremac'}\n",
            "{'serutcip', 'pictures'}\n",
            "{'quality', 'ytilauq', 'doog'}\n",
            "{'time'}\n",
            "{'even', 'neve'}\n",
            "{'osla', 'also'}\n",
            "{'hcum', 'much'}\n",
            "{'would', 'dluow'}\n",
            "{'nez', 'zen'}\n",
            "{'notron', 'norton'}\n",
            "{'router', 'retuor'}\n",
            "{'life', 'efil'}\n",
            "{'3pm', 'mp3'}\n",
            "{'product', 'tcudorp'}\n",
            "{'little', 'one'}\n",
            "{'deen'}\n",
            "{'size', 'ezis'}\n",
            "{'yrettab', 'battery'}\n",
            "{'want'}\n",
            "{'ecirp'}\n",
            "{'ysae', 'easy'}\n",
            "{'gnisu'}\n",
            "{'emit'}\n",
            "{'good', 'great', 'taerg'}\n",
            "{'eno'}\n",
            "{'need'}\n",
            "{'music', 'cisum'}\n",
            "{'desu'}\n",
            "{'teg'}\n",
            "Iteration 1 Accuracy: 0.74\n",
            "{'pmahc', 'champ', 'diaper', 'repaid'}\n",
            "{'3pm', 'music', 'cisum', 'mp3'}\n",
            "{'elttil', 'price', 'ecirp'}\n",
            "{'osla', 'also'}\n",
            "{'gnisu', 'use', 'esu'}\n",
            "{'krow', 'work'}\n",
            "{'ekil', 'need'}\n",
            "{'problem', 'melborp'}\n",
            "{'want', 'deen'}\n",
            "{'llits', 'still'}\n",
            "{'retteb', 'better'}\n",
            "{'great', 'taerg'}\n",
            "{'features', 'serutaef'}\n",
            "{'nez', 'zen', 'player'}\n",
            "{'ipod', 'dopi'}\n",
            "{'good', 'doog'}\n",
            "{'skrow', 'works'}\n",
            "{'reyalp', 'product'}\n",
            "{'hcum', 'much'}\n",
            "{'retupmoc', 'computer'}\n",
            "{'would', 'dluow'}\n",
            "{'get', 'teg'}\n",
            "{'life', 'efil'}\n",
            "{'first', 'tsrif'}\n",
            "{'evitaerc', 'creative'}\n",
            "{'size', 'ezis'}\n",
            "{'using'}\n",
            "{'serutcip', 'pictures'}\n",
            "{'yllaer', 'really'}\n",
            "{'emit', 'time'}\n",
            "{'ysae', 'easy'}\n",
            "{'llew', 'well'}\n",
            "{'enohp', 'phone'}\n",
            "{'neve'}\n",
            "{'camera', 'aremac'}\n",
            "{'eno', 'one'}\n",
            "{'yub', 'buy'}\n",
            "{'router', 'retuor'}\n",
            "{'used', 'desu'}\n",
            "{'yrettab', 'battery'}\n",
            "{'tnaw'}\n",
            "{'even'}\n",
            "{'tcudorp'}\n",
            "{'erawtfos', 'software'}\n",
            "{'sound', 'dnuos'}\n",
            "{'quality', 'ytilauq'}\n",
            "{'micro', 'orcim'}\n",
            "{'notron', 'norton'}\n",
            "{'like'}\n",
            "{'little'}\n",
            "Iteration 2 Accuracy: 0.84\n",
            "{'hcum', 'also', 'much'}\n",
            "{'dopi', 'player', 'reyalp'}\n",
            "{'gnisu', 'retupmoc', 'computer'}\n",
            "{'llits', 'really'}\n",
            "{'great', 'llew', 'well', 'taerg'}\n",
            "{'emit', 'time'}\n",
            "{'champ', 'product'}\n",
            "{'price', 'ecirp'}\n",
            "{'neve', 'ipod'}\n",
            "{'get', 'teg'}\n",
            "{'micro', 'orcim', 'evitaerc', 'creative'}\n",
            "{'features', 'desu'}\n",
            "{'nez', 'zen'}\n",
            "{'elttil', 'little'}\n",
            "{'yub', 'buy'}\n",
            "{'sound', 'dnuos'}\n",
            "{'skrow', 'works', 'work'}\n",
            "{'good', 'doog'}\n",
            "{'yllaer', 'osla'}\n",
            "{'want', 'tnaw'}\n",
            "{'use', 'esu'}\n",
            "{'serutcip', 'pictures'}\n",
            "{'camera', 'aremac'}\n",
            "{'first', 'tsrif'}\n",
            "{'problem', 'melborp'}\n",
            "{'still'}\n",
            "{'retteb', 'better'}\n",
            "{'would', 'dluow'}\n",
            "{'3pm', 'mp3'}\n",
            "{'like', 'ekil'}\n",
            "{'erawtfos', 'software'}\n",
            "{'notron', 'norton'}\n",
            "{'life', 'efil'}\n",
            "{'size', 'ezis'}\n",
            "{'quality', 'ytilauq'}\n",
            "{'even'}\n",
            "{'pmahc', 'diaper', 'repaid'}\n",
            "{'eno', 'one'}\n",
            "{'yrettab', 'battery'}\n",
            "{'router', 'retuor'}\n",
            "{'ysae', 'easy'}\n",
            "{'serutaef'}\n",
            "{'using'}\n",
            "{'deen'}\n",
            "{'enohp', 'phone'}\n",
            "{'krow'}\n",
            "{'used'}\n",
            "{'need'}\n",
            "{'tcudorp'}\n",
            "{'music', 'cisum'}\n",
            "Iteration 3 Accuracy: 0.76\n",
            "{'pmahc', 'champ', 'diaper', 'repaid'}\n",
            "{'yub', 'still', 'buy'}\n",
            "{'want', 'need'}\n",
            "{'serutcip', 'pictures'}\n",
            "{'nez', 'zen', 'much'}\n",
            "{'like', 'hcum'}\n",
            "{'good', 'great', 'taerg'}\n",
            "{'features', 'serutaef'}\n",
            "{'ipod', 'dopi'}\n",
            "{'get', 'teg'}\n",
            "{'gnisu', '3pm', 'mp3'}\n",
            "{'tnaw', 'ekil'}\n",
            "{'eno', 'one'}\n",
            "{'product', 'tcudorp'}\n",
            "{'used', 'desu'}\n",
            "{'would', 'dluow'}\n",
            "{'elttil', 'little'}\n",
            "{'price', 'ecirp'}\n",
            "{'ysae', 'easy'}\n",
            "{'micro', 'orcim'}\n",
            "{'neve', 'really'}\n",
            "{'problem', 'melborp'}\n",
            "{'osla', 'also'}\n",
            "{'emit', 'time'}\n",
            "{'first', 'tsrif'}\n",
            "{'erawtfos', 'software'}\n",
            "{'quality', 'ytilauq'}\n",
            "{'size', 'ezis'}\n",
            "{'skrow', 'works', 'work'}\n",
            "{'llew', 'well'}\n",
            "{'notron', 'norton'}\n",
            "{'sound', 'dnuos'}\n",
            "{'music', 'cisum'}\n",
            "{'retteb', 'better'}\n",
            "{'player', 'reyalp'}\n",
            "{'use', 'esu'}\n",
            "{'yrettab', 'battery'}\n",
            "{'retupmoc', 'computer'}\n",
            "{'evitaerc', 'creative'}\n",
            "{'enohp', 'phone'}\n",
            "{'life', 'efil'}\n",
            "{'llits'}\n",
            "{'camera', 'aremac'}\n",
            "{'krow'}\n",
            "{'even'}\n",
            "{'router', 'retuor'}\n",
            "{'doog'}\n",
            "{'using'}\n",
            "{'deen'}\n",
            "{'yllaer'}\n",
            "Iteration 4 Accuracy: 0.8\n",
            "{'like', 'ekil'}\n",
            "{'sound', 'dnuos'}\n",
            "{'llits', 'elttil'}\n",
            "{'emit', 'used', 'deen', 'ecirp'}\n",
            "{'pmahc', 'champ', 'diaper', 'repaid'}\n",
            "{'still', 'price'}\n",
            "{'get', 'teg'}\n",
            "{'good', 'great', 'doog', 'taerg'}\n",
            "{'even', 'neve'}\n",
            "{'retteb', 'better'}\n",
            "{'skrow', 'works', 'work'}\n",
            "{'product', 'tcudorp'}\n",
            "{'problem', 'melborp'}\n",
            "{'erawtfos', 'software'}\n",
            "{'router', 'retuor'}\n",
            "{'features', 'serutaef'}\n",
            "{'osla', 'really', 'also'}\n",
            "{'enohp', 'phone'}\n",
            "{'use', 'esu', 'using'}\n",
            "{'life', 'efil'}\n",
            "{'want'}\n",
            "{'retupmoc', 'computer'}\n",
            "{'serutcip', 'pictures'}\n",
            "{'yub', 'buy'}\n",
            "{'eno', 'one'}\n",
            "{'ysae', 'easy'}\n",
            "{'would', 'dluow'}\n",
            "{'llew', 'well'}\n",
            "{'evitaerc', 'creative'}\n",
            "{'first', 'tsrif'}\n",
            "{'camera', 'aremac'}\n",
            "{'gnisu'}\n",
            "{'3pm', 'mp3'}\n",
            "{'quality', 'ytilauq'}\n",
            "{'little'}\n",
            "{'time'}\n",
            "{'player', 'reyalp'}\n",
            "{'hcum', 'much'}\n",
            "{'yrettab', 'battery'}\n",
            "{'desu'}\n",
            "{'size', 'ezis'}\n",
            "{'yllaer'}\n",
            "{'krow'}\n",
            "{'notron', 'norton'}\n",
            "{'tnaw'}\n",
            "{'nez', 'zen'}\n",
            "{'need'}\n",
            "{'ipod', 'dopi'}\n",
            "{'micro', 'orcim'}\n",
            "{'music', 'cisum'}\n",
            "Iteration 5 Accuracy: 0.8\n",
            "Best performance with preprocessing:  ('Average accuracy:', 0.7879999999999999, 'Standard diviation:', 0.03487119154832538)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'elttil', 'really'}\n",
        "{'micro', 'orcim', 'evitaerc', 'creative'}\n",
        "{'llits', 'price'}\n",
        "{'use', 'esu', 'using'}\n",
        "{'erawtfos', 'software'}\n",
        "{'pmahc', 'champ', 'diaper', 'repaid'}\n",
        "{'retupmoc', 'computer'}\n",
        "{'yllaer', 'still'}\n",
        "{'player', 'reyalp'}\n",
        "{'tnaw', 'ekil'}\n",
        "{'like', 'used'}\n",
        "{'get', 'yub', 'buy'}\n",
        "{'skrow', 'works', 'krow', 'work'}\n",
        "{'problem', 'melborp'}\n",
        "{'enohp', 'phone'}\n",
        "{'llew', 'well'}\n",
        "{'features', 'serutaef'}\n",
        "{'sound', 'dnuos'}\n",
        "{'first', 'tsrif'}\n",
        "{'retteb', 'better'}\n",
        "{'ipod', 'dopi'}\n",
        "{'camera', 'aremac'}\n",
        "{'serutcip', 'pictures'}\n",
        "{'quality', 'ytilauq', 'doog'}\n",
        "{'time'}\n",
        "{'even', 'neve'}\n",
        "{'osla', 'also'}\n",
        "{'hcum', 'much'}\n",
        "{'would', 'dluow'}\n",
        "{'nez', 'zen'}\n",
        "{'notron', 'norton'}\n",
        "{'router', 'retuor'}\n",
        "{'life', 'efil'}\n",
        "{'3pm', 'mp3'}\n",
        "{'product', 'tcudorp'}\n",
        "{'little', 'one'}\n",
        "{'deen'}\n",
        "{'size', 'ezis'}\n",
        "{'yrettab', 'battery'}\n",
        "{'want'}\n",
        "{'ecirp'}\n",
        "{'ysae', 'easy'}\n",
        "{'gnisu'}\n",
        "{'emit'}\n",
        "{'good', 'great', 'taerg'}\n",
        "{'eno'}\n",
        "{'need'}\n",
        "{'music', 'cisum'}\n",
        "{'desu'}\n",
        "{'teg'}<br>\n",
        "Iteration 1 Accuracy: 0.74<br>\n",
        "{'pmahc', 'champ', 'diaper', 'repaid'}\n",
        "{'3pm', 'music', 'cisum', 'mp3'}\n",
        "{'elttil', 'price', 'ecirp'}\n",
        "{'osla', 'also'}\n",
        "{'gnisu', 'use', 'esu'}\n",
        "{'krow', 'work'}\n",
        "{'ekil', 'need'}\n",
        "{'problem', 'melborp'}\n",
        "{'want', 'deen'}\n",
        "{'llits', 'still'}\n",
        "{'retteb', 'better'}\n",
        "{'great', 'taerg'}\n",
        "{'features', 'serutaef'}\n",
        "{'nez', 'zen', 'player'}\n",
        "{'ipod', 'dopi'}\n",
        "{'good', 'doog'}\n",
        "{'skrow', 'works'}\n",
        "{'reyalp', 'product'}\n",
        "{'hcum', 'much'}\n",
        "{'retupmoc', 'computer'}\n",
        "{'would', 'dluow'}\n",
        "{'get', 'teg'}\n",
        "{'life', 'efil'}\n",
        "{'first', 'tsrif'}\n",
        "{'evitaerc', 'creative'}\n",
        "{'size', 'ezis'}\n",
        "{'using'}\n",
        "{'serutcip', 'pictures'}\n",
        "{'yllaer', 'really'}\n",
        "{'emit', 'time'}\n",
        "{'ysae', 'easy'}\n",
        "{'llew', 'well'}\n",
        "{'enohp', 'phone'}\n",
        "{'neve'}\n",
        "{'camera', 'aremac'}\n",
        "{'eno', 'one'}\n",
        "{'yub', 'buy'}\n",
        "{'router', 'retuor'}\n",
        "{'used', 'desu'}\n",
        "{'yrettab', 'battery'}\n",
        "{'tnaw'}\n",
        "{'even'}\n",
        "{'tcudorp'}\n",
        "{'erawtfos', 'software'}\n",
        "{'sound', 'dnuos'}\n",
        "{'quality', 'ytilauq'}\n",
        "{'micro', 'orcim'}\n",
        "{'notron', 'norton'}\n",
        "{'like'}\n",
        "{'little'}<br>\n",
        "Iteration 2 Accuracy: 0.84<br>\n",
        "{'hcum', 'also', 'much'}\n",
        "{'dopi', 'player', 'reyalp'}\n",
        "{'gnisu', 'retupmoc', 'computer'}\n",
        "{'llits', 'really'}\n",
        "{'great', 'llew', 'well', 'taerg'}\n",
        "{'emit', 'time'}\n",
        "{'champ', 'product'}\n",
        "{'price', 'ecirp'}\n",
        "{'neve', 'ipod'}\n",
        "{'get', 'teg'}\n",
        "{'micro', 'orcim', 'evitaerc', 'creative'}\n",
        "{'features', 'desu'}\n",
        "{'nez', 'zen'}\n",
        "{'elttil', 'little'}\n",
        "{'yub', 'buy'}\n",
        "{'sound', 'dnuos'}\n",
        "{'skrow', 'works', 'work'}\n",
        "{'good', 'doog'}\n",
        "{'yllaer', 'osla'}\n",
        "{'want', 'tnaw'}\n",
        "{'use', 'esu'}\n",
        "{'serutcip', 'pictures'}\n",
        "{'camera', 'aremac'}\n",
        "{'first', 'tsrif'}\n",
        "{'problem', 'melborp'}\n",
        "{'still'}\n",
        "{'retteb', 'better'}\n",
        "{'would', 'dluow'}\n",
        "{'3pm', 'mp3'}\n",
        "{'like', 'ekil'}\n",
        "{'erawtfos', 'software'}\n",
        "{'notron', 'norton'}\n",
        "{'life', 'efil'}\n",
        "{'size', 'ezis'}\n",
        "{'quality', 'ytilauq'}\n",
        "{'even'}\n",
        "{'pmahc', 'diaper', 'repaid'}\n",
        "{'eno', 'one'}\n",
        "{'yrettab', 'battery'}\n",
        "{'router', 'retuor'}\n",
        "{'ysae', 'easy'}\n",
        "{'serutaef'}\n",
        "{'using'}\n",
        "{'deen'}\n",
        "{'enohp', 'phone'}\n",
        "{'krow'}\n",
        "{'used'}\n",
        "{'need'}\n",
        "{'tcudorp'}\n",
        "{'music', 'cisum'}<br>\n",
        "Iteration 3 Accuracy: 0.76<br>\n",
        "{'pmahc', 'champ', 'diaper', 'repaid'}\n",
        "{'yub', 'still', 'buy'}\n",
        "{'want', 'need'}\n",
        "{'serutcip', 'pictures'}\n",
        "{'nez', 'zen', 'much'}\n",
        "{'like', 'hcum'}\n",
        "{'good', 'great', 'taerg'}\n",
        "{'features', 'serutaef'}\n",
        "{'ipod', 'dopi'}\n",
        "{'get', 'teg'}\n",
        "{'gnisu', '3pm', 'mp3'}\n",
        "{'tnaw', 'ekil'}\n",
        "{'eno', 'one'}\n",
        "{'product', 'tcudorp'}\n",
        "{'used', 'desu'}\n",
        "{'would', 'dluow'}\n",
        "{'elttil', 'little'}\n",
        "{'price', 'ecirp'}\n",
        "{'ysae', 'easy'}\n",
        "{'micro', 'orcim'}\n",
        "{'neve', 'really'}\n",
        "{'problem', 'melborp'}\n",
        "{'osla', 'also'}\n",
        "{'emit', 'time'}\n",
        "{'first', 'tsrif'}\n",
        "{'erawtfos', 'software'}\n",
        "{'quality', 'ytilauq'}\n",
        "{'size', 'ezis'}\n",
        "{'skrow', 'works', 'work'}\n",
        "{'llew', 'well'}\n",
        "{'notron', 'norton'}\n",
        "{'sound', 'dnuos'}\n",
        "{'music', 'cisum'}\n",
        "{'retteb', 'better'}\n",
        "{'player', 'reyalp'}\n",
        "{'use', 'esu'}\n",
        "{'yrettab', 'battery'}\n",
        "{'retupmoc', 'computer'}\n",
        "{'evitaerc', 'creative'}\n",
        "{'enohp', 'phone'}\n",
        "{'life', 'efil'}\n",
        "{'llits'}\n",
        "{'camera', 'aremac'}\n",
        "{'krow'}\n",
        "{'even'}\n",
        "{'router', 'retuor'}\n",
        "{'doog'}\n",
        "{'using'}\n",
        "{'deen'}\n",
        "{'yllaer'}<br>\n",
        "Iteration 4 Accuracy: 0.8<br>\n",
        "{'like', 'ekil'}\n",
        "{'sound', 'dnuos'}\n",
        "{'llits', 'elttil'}\n",
        "{'emit', 'used', 'deen', 'ecirp'}\n",
        "{'pmahc', 'champ', 'diaper', 'repaid'}\n",
        "{'still', 'price'}\n",
        "{'get', 'teg'}\n",
        "{'good', 'great', 'doog', 'taerg'}\n",
        "{'even', 'neve'}\n",
        "{'retteb', 'better'}\n",
        "{'skrow', 'works', 'work'}\n",
        "{'product', 'tcudorp'}\n",
        "{'problem', 'melborp'}\n",
        "{'erawtfos', 'software'}\n",
        "{'router', 'retuor'}\n",
        "{'features', 'serutaef'}\n",
        "{'osla', 'really', 'also'}\n",
        "{'enohp', 'phone'}\n",
        "{'use', 'esu', 'using'}\n",
        "{'life', 'efil'}\n",
        "{'want'}\n",
        "{'retupmoc', 'computer'}\n",
        "{'serutcip', 'pictures'}\n",
        "{'yub', 'buy'}\n",
        "{'eno', 'one'}\n",
        "{'ysae', 'easy'}\n",
        "{'would', 'dluow'}\n",
        "{'llew', 'well'}\n",
        "{'evitaerc', 'creative'}\n",
        "{'first', 'tsrif'}\n",
        "{'camera', 'aremac'}\n",
        "{'gnisu'}\n",
        "{'3pm', 'mp3'}\n",
        "{'quality', 'ytilauq'}\n",
        "{'little'}\n",
        "{'time'}\n",
        "{'player', 'reyalp'}\n",
        "{'hcum', 'much'}\n",
        "{'yrettab', 'battery'}\n",
        "{'desu'}\n",
        "{'size', 'ezis'}\n",
        "{'yllaer'}\n",
        "{'krow'}\n",
        "{'notron', 'norton'}\n",
        "{'tnaw'}\n",
        "{'nez', 'zen'}\n",
        "{'need'}\n",
        "{'ipod', 'dopi'}\n",
        "{'micro', 'orcim'}\n",
        "{'music', 'cisum'}<br>\n",
        "Iteration 5 Accuracy: 0.8<br>\n",
        "Best performance with preprocessing:  ('Average accuracy:', 0.7879999999999999, 'Standard diviation:', 0.03487119154832538)"
      ],
      "metadata": {
        "id": "VsUJsmRB9I3W"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcM1ICdyqHmT",
        "outputId": "f1d5ec5b-41fb-4287-d8ac-26811ffaba93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import re\n",
        "from random import sample, shuffle\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords, wordnet, words\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.metrics.distance import jaccard_distance, edit_distance\n",
        "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "\n",
        "# for use in removing stop words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# required for pos tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# required for lemmatization\n",
        "nltk.download('wordnet')\n",
        "# required for wordnet\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "print(torch.cuda.device_count())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO5sO3tar83f",
        "outputId": "0bbc310f-a92d-407e-cbf0-9582eeeba52a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "stemmer = nltk.SnowballStemmer(\"english\", ignore_stopwords = False)\n",
        "# NB path AND corpus_after_token_reversal SHOULD BE CHANGED TO MATCH\n",
        "# THE CORPUS PATH ON THE SPECIFIC MACHINE\n",
        "\n",
        "# Folder path for the corpus\n",
        "corpus_path = \"drive/MyDrive/34711-Cwk-S-DeepLearning_Minjun/product_reviews\"\n",
        "\n",
        "# Folder path where the reverse token corpus should be stored\n",
        "corpus_after_token_reversal = r\"drive/MyDrive/34711-Cwk-S-DeepLearning_Minjun/processed_reviews\"\n",
        "file_pattern = r\".*\"\n",
        "original_corpus = nltk.corpus.PlaintextCorpusReader(corpus_path, file_pattern)\n",
        "print(original_corpus.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yogD6emq5PQ",
        "outputId": "ab8c70de-b390-4e1e-9ee0-d715f240b480"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Canon_PowerShot_SD500.txt', 'Canon_S100.txt', 'Diaper_Champ.txt', 'Hitachi_router.txt', 'Linksys_Router.txt', 'MicroMP3.txt', 'Nokia_6600.txt', 'ipod.txt', 'norton.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core utility function for document cleaning\n",
        "# Works recursively, split the text into sentences/review, then for each sentence/review perform cleaning \n",
        "def process_doc(text, remove_punctuation, case_fold, stem,\n",
        "                remove_stopwords, remove_short_tokens, tokenize_by, manual_remove_list = [],\n",
        "                remove_nonalphabetical = False):\n",
        "\n",
        "  if (tokenize_by == \"sentence\"):\n",
        "    sentences = nltk.RegexpTokenizer(\"##\", gaps = True).tokenize(text)\n",
        "    sentences = [process_doc(sentence, remove_punctuation, case_fold, stem, \n",
        "                             remove_stopwords, remove_short_tokens, \"words\", manual_remove_list) \n",
        "                  for sentence in sentences]\n",
        "    return sentences\n",
        "  if (tokenize_by == \"reviews\"):\n",
        "    reviews = nltk.RegexpTokenizer(\"\\[t\\]\", gaps = True).tokenize(text)\n",
        "    reviews = [process_doc(review, remove_punctuation, case_fold, stem, \n",
        "                              remove_stopwords, remove_short_tokens, \"words\", manual_remove_list)\n",
        "                for review in reviews]\n",
        "    return reviews\n",
        "  if (tokenize_by == \"words\"):\n",
        "    words = nltk.TreebankWordTokenizer().tokenize(text)\n",
        "    if (remove_punctuation):\n",
        "      words = [w for w in words if w not in string.punctuation and w != \"...\" and w != \"]##\"\n",
        "                and w != \"'s\"\n",
        "                and w != \"'m\"\n",
        "                and w != \"'re\"\n",
        "                and w != \"'ve\"]\n",
        "      words = [w.strip(\"\") for w in words]\n",
        "      words = [w.strip(\".\") for w in words]\n",
        "    if (case_fold):\n",
        "      words = [w.lower() for w in words]\n",
        "    if (remove_short_tokens):\n",
        "      words = [w for w in words if len(w) > 2]\n",
        "    if (stem):\n",
        "      words = [w if w in manual_remove_list else stemmer.stem(w) for w in words]\n",
        "    if (remove_stopwords):\n",
        "      words = [w for w in words if w not in stop_words and w != \"n't\"]\n",
        "    if (remove_punctuation):\n",
        "      words = [w for w in words if w not in string.punctuation and w != \"...\" and w != \"]##\"]\n",
        "    if (remove_nonalphabetical):\n",
        "      words = [w for w in words if w.isalpha()]\n",
        "    return words\n",
        "\n",
        "def process_corpus(corpus, remove_punctuation:bool, case_fold:bool, stem:bool,\n",
        "                  remove_stopwords:bool, remove_short_tokens, tokenize_by:str, remove_nonalphabetical):\n",
        "  docs = [word for fileid in corpus.fileids() \n",
        "            for word in process_doc(corpus.raw(fileid), remove_punctuation, case_fold,\n",
        "                                    stem, remove_stopwords, remove_short_tokens, \n",
        "                                    tokenize_by, remove_nonalphabetical)\n",
        "         ]\n",
        "  return docs\n",
        "\n",
        "def most_frequent(words, n, should_print):\n",
        "  freqDist = nltk.FreqDist(words)\n",
        "  most_common = freqDist.most_common(n)\n",
        "  if (should_print):\n",
        "    i = 1\n",
        "    for (w, count) in most_common:\n",
        "      print(i , w , count)\n",
        "      i += 1\n",
        "  return most_common"
      ],
      "metadata": {
        "id": "LukfxSKns4q3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_sentences_cleaned(corpus_filepath):\n",
        "  corpus = nltk.corpus.PlaintextCorpusReader(corpus_filepath, file_pattern)\n",
        "  out = []\n",
        "  for fileid in corpus.fileids():\n",
        "    # removing some punctuation manually\n",
        "    sentences = process_doc(corpus.raw(fileid), True, True, True, True, True,\"sentence\", [\"§\", \"―\",\"•\",\"\\t\",\"←\",\"→\"], False)\n",
        "    out.extend(sentences)\n",
        "  return out\n",
        "\n",
        "# partitions corpus into sentiments, and cleans the text\n",
        "def clean_all_sentiments(corpus_filepath, stemming, stop_words, remove_shuffled_sentiments = False):\n",
        "  corpus = nltk.corpus.PlaintextCorpusReader(corpus_filepath, file_pattern)\n",
        "\n",
        "\n",
        "  # pattern used to match sentiments == words before ##\n",
        "  # ex) sound[+2] ##\n",
        "  pattern = re.compile(r\"(([a-z -]*\\[[\\-\\+][0-9]\\],? ?)+#[^(\\[)]+)\")\n",
        "  sentiments = []\n",
        "  for file_id in corpus.fileids():\n",
        "    text = corpus.raw(file_id)\n",
        "    # [cs],[t] replaced into empty string\n",
        "    text = re.sub(\"\\[[a-z]+\\]\", \"\", text)\n",
        "    text = pattern.findall(text)\n",
        "    for sentiment in text:\n",
        "      sentiment_parsed = sentiment[0]\n",
        "      # Find all labels for whether a sentiment is positive or negative\n",
        "      # ex) [+1],[-1]\n",
        "      matches = re.findall(\"\\[[\\+\\-][0-9]\\]\", sentiment_parsed)\n",
        "      score = 0\n",
        "      has_positive = False\n",
        "      has_negative = False\n",
        "      for match in matches:\n",
        "        score += int(match[1:-1])\n",
        "        if match[1] == \"+\":\n",
        "          has_positive = True\n",
        "        if match[1] == \"-\":\n",
        "          has_negative = True\n",
        "      # if the sum of all scores is 0 discard the sample, since we are doing binary \n",
        "      # classification, optionally remove all sentiments with mixed labels\n",
        "      if remove_shuffled_sentiments and has_positive and has_negative:\n",
        "        continue\n",
        "      if (score == 0): continue\n",
        "      if (score < 0): score = 0\n",
        "      if (score > 0): score = 1\n",
        "      sentiment_parsed = process_doc(sentiment_parsed, True, True, stemming, stop_words, True, \"words\", [], True)\n",
        "      if (len(sentiment_parsed[:-1]) < 2): continue\n",
        "      sentiments.append((sentiment_parsed[:-1], score))\n",
        "  return sentiments\n",
        "\n",
        "def generate_word_to_indx_and_idx_to_word(corpus):\n",
        "  word_to_idx = {}\n",
        "  idx_to_word = {}\n",
        "  i = 0\n",
        "  for sentence in corpus:\n",
        "    for word in sentence[0]:\n",
        "      if (word not in word_to_idx):\n",
        "        word_to_idx[word] = i\n",
        "        idx_to_word[i] = word\n",
        "        i += 1\n",
        "  return (word_to_idx, idx_to_word)\n",
        "\n",
        "def get_context_window_tuples(word_to_idx, sentences, window, key_words):\n",
        "  tuples = []\n",
        "  for sentence in sentences:\n",
        "    for i in range(window, len(sentence) - window):\n",
        "      # if sentence[i] in key_words:\n",
        "        context = []\n",
        "        middle_word = word_to_idx[sentence[i]]\n",
        "        for j in range (i - window, i + window + 1):\n",
        "          if i != j:\n",
        "            context.append(word_to_idx[sentence[j]])\n",
        "        tuples.append((context, word_to_idx[sentence[i]]))\n",
        "          \n",
        "        \n",
        "  return tuples\n",
        "\n",
        "\n",
        "def get_skipgrams(sentiments, window):\n",
        "  word = []\n",
        "  context = []\n",
        "  for sentiment in sentiments:\n",
        "    sentence = sentiment[0]\n",
        "    for i in range(len(sentence)):\n",
        "      cont = [sentence[idx] for idx in range(max(0, i - window), min(len(sentence), i + window + 1)) if idx != i]\n",
        "      word.extend([sentence[i]] * (len(cont)))\n",
        "      context.extend(cont)\n",
        "  return(word, context)     \n",
        "\n",
        "def get_batches(words, contexts, batch_size):\n",
        "  shuffled_idxs = sample(range(0, len(words)), len(words))\n",
        "  batches = []\n",
        "\n",
        "  batch_word, batch_context = [], []\n",
        "  for i in range(len(words)):\n",
        "    idx = shuffled_idxs[i]\n",
        "    batch_word.append(words[idx])\n",
        "    batch_context.append(contexts[idx])\n",
        "    if (i + 1) % batch_size == 0 or i + 1 == len(words):\n",
        "      batches.append((\n",
        "        torch.from_numpy(np.array(batch_word)),\n",
        "        torch.from_numpy(np.array(batch_context))\n",
        "      ))\n",
        "      batch_word, batch_context = [], []\n",
        "  return batches\n",
        "  \n",
        "def get_x_tensors(x_y_tuples):\n",
        "  tensors = []\n",
        "  for tuple in x_y_tuples:\n",
        "    tensors.append(torch.tensor(tuple[0], dtype=torch.long))\n",
        "  return tensors\n",
        "\n",
        "\n",
        "def get_y_tensors(tuples, num_classes):\n",
        "  tensors = []\n",
        "  \n",
        "  for tuple in tuples:\n",
        "    tensors.append(F.one_hot(torch.tensor(tuple[1]), num_classes=num_classes))\n",
        "  return tensors\n",
        "\n",
        "def get_sentiments_as_word_idxs(sentiments, word_to_idx):\n",
        "  return [([word_to_idx[word] for word in words], label) for (words, label) in sentiments]"
      ],
      "metadata": {
        "id": "6o-sMLFWtNk7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to split data into K folds\n",
        "def k_fold_partititoning(sentiments, k, DO_SHUFFLE):\n",
        "  # shuffle\n",
        "  # we do not shuffle when we need to compare the results of experiments\n",
        "  if (DO_SHUFFLE):\n",
        "    shuffle(sentiments)\n",
        "  folds = []\n",
        "  # determine fold size\n",
        "  partition_step = len(sentiments) // k\n",
        "  remainders = len(sentiments) % k\n",
        "  start = 0\n",
        "  # append to each fold\n",
        "  for i in range(k):\n",
        "    if (remainders > 0):\n",
        "      folds.append(sentiments[start : start + partition_step + 1])\n",
        "      start += partition_step + 1\n",
        "      remainders -= 0\n",
        "    else:\n",
        "      folds.append(sentiments[start : start + partition_step])\n",
        "      start += partition_step\n",
        "  return folds\n",
        "\n",
        "# partition the data into two - the i-th fold and the rest\n",
        "def split_training_testing_from_k_folds(i, folds):\n",
        "  # To ensure that not tampering is done \n",
        "  testing = copy.deepcopy(folds[i])\n",
        "  \n",
        "  training = []\n",
        "  for j in range(i):\n",
        "    training.extend(folds[j])\n",
        "  for j in range(i + 1, len(folds)):\n",
        "    training.extend(folds[j])\n",
        "  return (training, testing)\n",
        "  \n",
        "def to_tensors(sentiments):\n",
        "  shuffle(sentiments)\n",
        "  start = 0\n",
        "  batches = [(torch.from_numpy(np.array(sentiment[0])), torch.from_numpy(np.array(sentiment[1], dtype=float))) for sentiment in sentiments]\n",
        "  return batches\n",
        "\n",
        "# y_hat is a tensor output of a sigmoid (y_hat between: [0, 1])\n",
        "def get_binary_accuracy(y_hat, y, verbose=False):\n",
        "  # if y_hat <= 0.5: rounded = 0 else: rounded = 1\n",
        "  rounded = torch.round(y_hat)\n",
        "  correct = (rounded == y).float()\n",
        "  if verbose:\n",
        "    print(\"y_hat:\", y_hat.data)\n",
        "    print(\"y:\", y.data)\n",
        "    print(\"rounded:\", rounded.data)\n",
        "    print(\"correct: \", correct.data)\n",
        "  return correct"
      ],
      "metadata": {
        "id": "Zopcgfm7t2ru"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, FILTER_NUM, embed_dim = None, padding_idx = None, embedding_weights = None, dropout_rate = 0):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        if (embedding_weights != None):\n",
        "          self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=False)\n",
        "          embed_dim = embedding_weights.size()[1]\n",
        "        else:\n",
        "          self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx = padding_idx)\n",
        "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = FILTER_NUM, \n",
        "                                kernel_size = (3, embed_dim))\n",
        "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = FILTER_NUM, \n",
        "                                kernel_size = (4, embed_dim))\n",
        "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = FILTER_NUM, \n",
        "                                kernel_size = (5, embed_dim))\n",
        "        \n",
        "        self.fc = nn.Linear(3 * FILTER_NUM, 1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        \n",
        "    def forward(self, text, training = False):\n",
        "  \n",
        "        embedding = self.embedding(text)\n",
        "        #embedding = [len(text) x embedding_size] \n",
        "\n",
        "        embedding = embedding.unsqueeze(1)\n",
        "        embedding = embedding.unsqueeze(1)\n",
        "        embedding = embedding.permute(1, 2, 0, 3)\n",
        "\n",
        "        conved_0 = F.relu(self.conv_0(embedding).squeeze(3))\n",
        "        conved_1 = F.relu(self.conv_1(embedding).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedding).squeeze(3))\n",
        "        #conved_n = [len(text) - kernel_size x number of filters] \n",
        "        \n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        concat = torch.cat((\n",
        "            pooled_0, \n",
        "            pooled_1, \n",
        "            pooled_2)\n",
        "          , dim = 1)\n",
        "        # Apply dropout only when training\n",
        "        if (training):\n",
        "          concat = self.dropout(concat)\n",
        "        \n",
        "        #concat = [len(text) - kernel_size x number of filters] \n",
        "        return self.sigmoid(self.fc(concat))"
      ],
      "metadata": {
        "id": "wLYNQJGBt3i0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(FILTER_NUM, EMBEDDING_DIMENSION, epochs, batch_size, learning_rate, folds, vocab_size, padding_idx, dropout_rate, verbose):\n",
        "  K = len(folds)\n",
        "  # accuracies will contain the accuracies for each fold for each epoch\n",
        "  accuracies = np.zeros((K, epochs))\n",
        "  for k in range(K):\n",
        "    if (verbose):\n",
        "      print(\"FOLD: \", k + 1)\n",
        "    (training_data, testing_data) = split_training_testing_from_k_folds(k, folds)\n",
        "    model = CNN(vocab_size + 1, FILTER_NUM, EMBEDDING_DIMENSION, padding_idx, dropout_rate=dropout_rate)\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), learning_rate)\n",
        "    loss_fn = nn.BCELoss()\n",
        "    loss_fn = loss_fn.to(device)\n",
        "    for epoch in range(epochs):\n",
        "      if (verbose):\n",
        "        print(\"EPOCH:\", epoch + 1)\n",
        "      acc = 0\n",
        "      total_loss = 0\n",
        "      n = 0\n",
        "      for sample in to_tensors(training_data):\n",
        "        optimizer.zero_grad()\n",
        "        (sentiment, label) = sample \n",
        "        sentiment = sentiment.to(device)\n",
        "        label = label.to(device)\n",
        "        y_hat = model(sentiment, training = True).squeeze()\n",
        "        acc += get_binary_accuracy(y_hat, label, 1 == 0)\n",
        "        loss = loss_fn(y_hat, label.float())\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        n += 1\n",
        "      if (verbose):\n",
        "        print(\"TRAINING: accuracy\", (acc / n).item(), \"total loss\", total_loss.item())\n",
        "      with torch.no_grad():\n",
        "        accuracy = 0\n",
        "        for (sentiment, label) in to_tensors(testing_data):\n",
        "          sentiment = sentiment.to(device)\n",
        "          label = label.to(device)\n",
        "          accuracy += get_binary_accuracy(model(sentiment), label)\n",
        "        accuracies[k][epoch] = (accuracy / len(testing_data)).item()\n",
        "        if (verbose):\n",
        "          print(\"EPOCH VALIDATION ACCURACY\", accuracy.item())\n",
        "  \n",
        "  # epoch averages contains the average accuracy for each epoch accross all folds\n",
        "  epoch_averages = np.mean(accuracies, axis=0)\n",
        "  best_epoch = np.argmax(epoch_averages)\n",
        "  return \"Best epoch:\", best_epoch + 1, \"with an average\", epoch_averages[best_epoch]"
      ],
      "metadata": {
        "id": "hDdlExB8t6hz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(FILTER_NUM, EMBEDDING_DIMENSION, epochs, batch_size, learning_rate, stemming, stopword_removal, dropout, verbose, DO_SHUFFLE, remove_shuffled = False):\n",
        "  # sentiments - a list of tuples, tuple[0] is the cleaned text of a sentiment, tuple[1] is {0, 1} represents if a sentiment is positive(1) or negative(0)\n",
        "  sentiments = clean_all_sentiments(corpus_path, stemming, stopword_removal, remove_shuffled)\n",
        "  (word_to_idx, idx_to_word) = generate_word_to_indx_and_idx_to_word(sentiments)\n",
        "  # tuple[0] in sentiments becomes a list of ints, each int represents a token, word_to_idx, idx_to_word contain the mapping\n",
        "  sentiments = get_sentiments_as_word_idxs(sentiments, word_to_idx)\n",
        "  PADDING_STR = \"\"\n",
        "  PADDING_IDX = len(word_to_idx)\n",
        "  idx_to_word[PADDING_IDX] = PADDING_STR\n",
        "  word_to_idx[PADDING_STR] = PADDING_IDX\n",
        "  vocab_size = len(idx_to_word)\n",
        "  # The filter size of the CNN is 5, all shorter texts than that need padding\n",
        "  for sentiment in sentiments:\n",
        "    while (len(sentiment[0]) < 5): \n",
        "      sentiment[0].append(PADDING_IDX)\n",
        "  # 5-fold\n",
        "  k_folds = k_fold_partititoning(sentiments, 5, DO_SHUFFLE)\n",
        "  \n",
        "  # training and evaluation\n",
        "  return train_and_eval(FILTER_NUM, EMBEDDING_DIMENSION, epochs, batch_size, learning_rate, k_folds, vocab_size, PADDING_IDX, dropout, verbose)\n"
      ],
      "metadata": {
        "id": "VEhjdTDHt9Cs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Removing mixed sentiments\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 5, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.75, verbose = False, DO_SHUFFLE = True, remove_shuffled = True))\n",
        "\n",
        "print(\"Keeping mixed sentiments\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 5, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.75, verbose = False, DO_SHUFFLE = False, remove_shuffled = False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "VyG0NEAAuBgX",
        "outputId": "00c9d4ec-bdc3-4d41-936e-7129b647e680"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6a1664079ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m print(\"Removing mixed sentiments\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 5, batch_size = 150, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                       learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.75, verbose = False, DO_SHUFFLE = True, remove_shuffled = True))\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print(\"Keeping mixed sentiments\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 5, batch_size = 150, \n\u001b[1;32m      5\u001b[0m                                       learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.75, verbose = False, DO_SHUFFLE = False, remove_shuffled = False))\n",
            "\u001b[0;32m<ipython-input-26-03b4444a7d88>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(FILTER_NUM, EMBEDDING_DIMENSION, epochs, batch_size, learning_rate, stemming, stopword_removal, dropout, verbose, DO_SHUFFLE, remove_shuffled)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# training and evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILTER_NUM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIMENSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPADDING_IDX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-d1bd8f7fe9f4>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(FILTER_NUM, EMBEDDING_DIMENSION, epochs, batch_size, learning_rate, folds, vocab_size, padding_idx, dropout_rate, verbose)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing mixed sentiments ('Best epoch:', 5, 'with an average', 0.7034740567207336)<br>\n",
        "Keeping mixed sentiments ('Best epoch:', 3, 'with an average', 0.6950494885444641)"
      ],
      "metadata": {
        "id": "wNRL6cPeimVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After stemming\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=True, stopword_removal = False, dropout = 0.75, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"After removing stop words\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = True, dropout = 0.75, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Without both stemming and removing stop words\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.75, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"After both stemming and removing stop words\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=True, stopword_removal = True, dropout = 0.75, verbose = False, DO_SHUFFLE = False))"
      ],
      "metadata": {
        "id": "YhTsWQJDuF8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After stemming ('Best epoch:', 14, 'with an average', 0.7024752378463746)<br>\n",
        "After removing stop words ('Best epoch:', 5, 'with an average', 0.6800979614257813)<br>\n",
        "Without both stemming and removing stop words ('Best epoch:', 15, 'with an average', 0.6920791983604431)<br>\n",
        "After both stemming and removing stop words ('Best epoch:', 6, 'with an average', 0.6918968319892883)"
      ],
      "metadata": {
        "id": "WRjjhLG7mwqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dropout 0\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Dropout 0.25\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.25, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Dropout 0.50\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.50, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Dropout 0.75\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.75, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Dropout 0.85\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Dropout 0.95\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.95, verbose = False, DO_SHUFFLE = False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mAq6J8yuIsm",
        "outputId": "c7108e49-2234-42ac-e3d4-dc625ffd9e6e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout 0 ('Best epoch:', 15, 'with an average', 0.706930673122406)\n",
            "Dropout 0.25 ('Best epoch:', 8, 'with an average', 0.7123762130737304)\n",
            "Dropout 0.50 ('Best epoch:', 7, 'with an average', 0.6935643553733826)\n",
            "Dropout 0.75 ('Best epoch:', 8, 'with an average', 0.7089108824729919)\n",
            "Dropout 0.85 ('Best epoch:', 9, 'with an average', 0.6970296978950501)\n",
            "Dropout 0.95 ('Best epoch:', 11, 'with an average', 0.6707920789718628)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout 0 ('Best epoch:', 19, 'with an average', 0.7014851331710815)<br>\n",
        "Dropout 0.25 ('Best epoch:', 3, 'with an average', 0.6915841460227966)<br>\n",
        "Dropout 0.50 ('Best epoch:', 9, 'with an average', 0.7064356327056884)<br>\n",
        "Dropout 0.75 ('Best epoch:', 16, 'with an average', 0.6876237630844116)<br>\n",
        "Dropout 0.85 ('Best epoch:', 12, 'with an average', 0.6955445528030395)<br>\n",
        "Dropout 0.95 ('Best epoch:', 7, 'with an average', 0.6594059348106385)"
      ],
      "metadata": {
        "id": "ZzrlThm2m3B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running for only 15 epochs to speed up the experiment \n",
        "print(\"Filter number 50:\", run_experiment(FILTER_NUM = 50, EMBEDDING_DIMENSION = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Filter number 100\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Filter number 200\", run_experiment(FILTER_NUM = 200, EMBEDDING_DIMENSION = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Filter number 300\", run_experiment(FILTER_NUM = 300, EMBEDDING_DIMENSION = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Filter number 400\", run_experiment(FILTER_NUM = 400, EMBEDDING_DIMENSION = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = False))\n"
      ],
      "metadata": {
        "id": "XISCSiJzuNQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter number 50: ('Best epoch:', 9, 'with an average', 0.707920777797699)<br>\n",
        "Filter number 100 ('Best epoch:', 12, 'with an average', 0.6925742506980896)<br>\n",
        "Filter number 200 ('Best epoch:', 11, 'with an average', 0.6960396051406861)<br>\n",
        "Filter number 300 ('Best epoch:', 15, 'with an average', 0.6792079091072083)<br>\n",
        "Filter number 400 ('Best epoch:', 7, 'with an average', 0.662871265411377)<br>"
      ],
      "metadata": {
        "id": "mKkXd0vbnI1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding size 50\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 50, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Embedding size 100\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 100, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = False))\n",
        "\n",
        "print(\"Embedding size 200\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 200, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = False))\n",
        " \n",
        "print(\"Embedding size 300\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = False))\n",
        "  \n",
        "print(\"Embedding size 400\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 400, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = False))\n"
      ],
      "metadata": {
        "id": "_Km41GrquRg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding size 50 ('Best epoch:', 15, 'with an average', 0.701980185508728)<br>\n",
        "Embedding size 100 ('Best epoch:', 10, 'with an average', 0.7064356327056884)<br>\n",
        "Embedding size 200 ('Best epoch:', 14, 'with an average', 0.7059405684471131)<br>\n",
        "Embedding size 300 ('Best epoch:', 10, 'with an average', 0.6995049476623535)<br>\n",
        "Embedding size 400 ('Best epoch:', 19, 'with an average', 0.7089108824729919)<br>"
      ],
      "metadata": {
        "id": "QfDd13zKnLrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy with best parameters:\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 30, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, DO_SHUFFLE = True, remove_shuffled = True))\n"
      ],
      "metadata": {
        "id": "0wrv-gQuuU40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy with best parameters: ('Best epoch:', 8, 'with an average', 0.7395743727684021)"
      ],
      "metadata": {
        "id": "VJZhXnLbqpp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy with best parameters:\", run_experiment(FILTER_NUM = 100, EMBEDDING_DIMENSION = 300, epochs = 30, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=True, stopword_removal = True, dropout = 0.85, verbose = False, DO_SHUFFLE = True, remove_shuffled = True))"
      ],
      "metadata": {
        "id": "BXB0Tvxq1pNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy with best parameters and pre-proccessing: ('Best epoch:', 9, 'with an average', 0.7256854057312012)"
      ],
      "metadata": {
        "id": "995XGuZu7ZOL"
      }
    }
  ]
}
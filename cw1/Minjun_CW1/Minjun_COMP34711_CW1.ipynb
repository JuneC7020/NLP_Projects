{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKdMMQz1dg6S",
        "outputId": "d1690c39-c442-474b-d66e-dcb3d973a2c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkjW3f2VdnFz",
        "outputId": "109f5a25-1c69-4a38-9be7-0e7744dd034e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd drive/MyDrive/Colab\\ Notebooks\n",
        "# set path to /content/.... as absolute path instead?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-XJdYXPduZ_",
        "outputId": "9e09367e-c582-4864-aa78-817ab6ed13d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: cd: drive/MyDrive/Colab Notebooks: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E2vsxjjdtV4",
        "outputId": "87e118ff-5681-42d4-c69a-2ac489abe50e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import TweetTokenizer, MWETokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# for use in removing stop words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# required for pos tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# required for lemmatization\n",
        "nltk.download('wordnet')\n",
        "# required for wordnet\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zjhjVKMeTTQ",
        "outputId": "ef2d3bcb-1fa8-48fc-c417-60b22cffec5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A1BtqdRT-0mB"
      },
      "outputs": [],
      "source": [
        "class InvertedIndex:\n",
        "    \"\"\"\n",
        "    Construct Inverted Index\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize any instance variables that have to be set\n",
        "        \"\"\"\n",
        "        # Window size for Task 2, initial window size is 2 since we will be only searching with two terms\n",
        "        self.window_size = 2\n",
        "        # Using TweetTokenizer as initial tokenizer\n",
        "        # See report for justification\n",
        "        self.unigram_tokenizer = TweetTokenizer()\n",
        "        # Declaring mwe_tokenizer as instance var - is initialized later\n",
        "        self.mwe_tokenizer = None\n",
        "        # Initializing file map as empty dict\n",
        "        self.file_map = {}\n",
        "        # Initialize distionary that stores inverted index\n",
        "        self.inv_index = {}\n",
        "        # Set of stop words downloaded from nltk\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "        # Initializing WordNetLemmatizer\n",
        "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def pos_tagger(self, nltk_tag):\n",
        "        \"\"\"\n",
        "        Take a POS tag, and return a wordnet equivalent tag for use in lemmatization\n",
        "        \"\"\"\n",
        "        if nltk_tag.startswith(\"J\"):\n",
        "            return wordnet.ADJ\n",
        "        elif nltk_tag.startswith(\"V\"):\n",
        "            return wordnet.VERB\n",
        "        elif nltk_tag.startswith(\"N\"):\n",
        "            return wordnet.NOUN\n",
        "        elif nltk_tag.startswith(\"R\"):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "\n",
        "    def read_data(self, path: str) -> list:\n",
        "        \"\"\"\n",
        "        Read files from a directory and then append the data of each file into a list.\n",
        "        \"\"\"\n",
        "        # If path doesn't end with forwardslash, add it\n",
        "        if path[-1] != \"/\":\n",
        "            path = path + \"/\"\n",
        "\n",
        "        # Look through all the filenames, get any with .csv in name\n",
        "        files_list = sorted(\n",
        "            [filename for filename in os.listdir(path) if \".csv\" in filename]\n",
        "        )\n",
        "\n",
        "        # Initialize a list to store all terms from the csv files\n",
        "        special_tokens = []\n",
        "        # Iterate through each .csv file\n",
        "        for filename in files_list:\n",
        "            # read the contents of .csv file using the pandas library\n",
        "            names = pd.read_csv(path + filename)\n",
        "            # Take column 'name' and convert to list, append to apecial_tokens\n",
        "            names = names[\"name\"].to_list()\n",
        "            special_tokens = [*special_tokens, *names]\n",
        "\n",
        "        # Convert all special tokens into lowercase for case insensitivity\n",
        "        special_tokens = [token.lower() for token in special_tokens]\n",
        "        # Initialize the multi-word tokenizer with terms extracted from csv files\n",
        "        self.mwe_tokenizer = MWETokenizer([tuple(word.split(\" \")) for word in special_tokens])\n",
        "\n",
        "\n",
        "        # Look through all the filenames, get any with .txt in name\n",
        "        files_list = sorted(\n",
        "            [filename for filename in os.listdir(path) if \".txt\" in filename]\n",
        "        )\n",
        "\n",
        "        # Initialize a list to store lists of tokens extracted from the txt files\n",
        "        tokens_list = []\n",
        "        # file id starts from 0\n",
        "        file_id = 0\n",
        "        # Iterate through each .txt file\n",
        "        for filename in files_list:\n",
        "            # Extract the contents of the file as string\n",
        "            with open(path + filename, \"r\", encoding=\"utf8\") as file:\n",
        "                raw_text = file.read()\n",
        "            file.close()\n",
        "            # Map each file id with respective file name\n",
        "            self.file_map[file_id] = filename\n",
        "            # Convert raw text into all lowercase first\n",
        "            # then process_document with the extracted raw text\n",
        "            tokens = self.process_document(raw_text.lower())\n",
        "            # Take the processed tokens and append it to the list of tokens\n",
        "            tokens_list.append(tokens)\n",
        "            file_id += 1\n",
        "\n",
        "        # Return the full set of tokens generated from the text files\n",
        "        return tokens_list\n",
        "        \n",
        "    def process_document(self, document: str) -> list:\n",
        "        \"\"\"\n",
        "        pre-process a document and return a list of its terms\n",
        "        str->list\"\"\"\n",
        "\n",
        "        # Regex to remove any square brackets and any content inside them\n",
        "        # None of the square brackets contain meaningful words in our corpus\n",
        "        raw_text = re.sub(\"\\[.*?\\]\", \"\", document)\n",
        "\n",
        "        # Removing formatting text that appears in nearly every article\n",
        "        raw_text = raw_text.replace(\"From Wikipedia, the free encyclopedia\", \"\")\n",
        "        raw_text = raw_text.replace(\"Jump to navigationJump to search\", \"\")\n",
        "        raw_text = raw_text.replace(\"List of episodes\", \"\")\n",
        "        raw_text = raw_text.replace(\"Plot\", \"\")\n",
        "        raw_text = raw_text.replace(\"Production\", \"\")\n",
        "        raw_text = raw_text.replace(\"Reception\", \"\")\n",
        "        raw_text = raw_text.replace(\"References\", \"\")\n",
        "        raw_text = raw_text.replace(\"External links\", \"\")\n",
        "\n",
        "        # Make use of TweetTokenizer to split raw text\n",
        "        tokens_with_punctuations = self.unigram_tokenizer.tokenize(raw_text)\n",
        "        \n",
        "        # Removing punctucations and unnecessary symbols\n",
        "        # punctuations are lifted from string.punctuations\n",
        "        # Exceptions are ' # - and _\n",
        "        punctuations = '\\t←→!\"$%&()*+, ./:;<=>?@[\\]^`{|}~'\n",
        "\n",
        "        # We make use of a trans table to remove specified symbols\n",
        "        # By providing 3 arguments, we remove all occurrances of \n",
        "        # any characters present in the 3rd argument\n",
        "        trans_table = str.maketrans(\"\", \"\", punctuations)\n",
        "        stripped_words = [word.translate(trans_table) for word in tokens_with_punctuations]\n",
        "        \n",
        "        # We iterate through each token in stripped_words\n",
        "        # and remove any leftover empty strings\n",
        "        tokens_without_punctuations = [str for str in stripped_words if str]\n",
        "\n",
        "\n",
        "        ##### POS TAGGING\n",
        "        pos_tagged = nltk.pos_tag(tokens_without_punctuations)\n",
        "        wordnet_tagged = list(map(lambda x: (x[0], self.pos_tagger(x[1])), pos_tagged))\n",
        "\n",
        "        lemmatized = [\n",
        "            word if tag is None else self.wordnet_lemmatizer.lemmatize(word, tag)\n",
        "            for word, tag in wordnet_tagged\n",
        "        ]\n",
        "        #####\n",
        "\n",
        "        # Converting any occurrances of multi-word phrases from .csv files\n",
        "        # into singular, multi-word tokens\n",
        "        multi_tokens = self.mwe_tokenizer.tokenize(tokens_without_punctuations)\n",
        "        # Replacing '_' with whitespace for better presentation on output\n",
        "        multi_tokens = [word.replace(\"_\", \" \") for word in multi_tokens]\n",
        "        # Removing stopwords on multi-word tokens\n",
        "        # We replace all stop words with placeholders to keep the word position consistent\n",
        "        multi_tokens_without_sw = [word if not word in self.stop_words else \"<place_holder>\" for word in multi_tokens]\n",
        "        # Doing the same for unigram tokens\n",
        "        uni_tokens_without_sw = [word if not word in self.stop_words else \"<place_holder>\" for word in lemmatized]\n",
        "\n",
        "        # Return a list containing two lists of tokens\n",
        "        # One containing unigrams, one also containing multi-word tokens\n",
        "        token_tuple = [uni_tokens_without_sw, multi_tokens_without_sw]\n",
        "        return token_tuple\n",
        "    \n",
        "    def index_corpus(self, documents: list) -> None:\n",
        "        \"\"\"\n",
        "        index given documents\n",
        "        list->None\"\"\"\n",
        "\n",
        "        # Iterate through each set of tokens extracted from documents\n",
        "        #inv_index[term][0]-> incidences of documents of the term / inv_index[term][1]-> file id , postions appended\n",
        "        for file_id, token_tuple in enumerate(documents):\n",
        "            # Iterate through each unigram tokens(token_tuple[0])\n",
        "            for position, term in enumerate(token_tuple[0]):\n",
        "                # Skip any occurrances of placeholder tokens\n",
        "                if term != \"<place_holder>\":\n",
        "                    # If term has occurred before\n",
        "                    if term in self.inv_index:\n",
        "                        # If term has occurred before in current document\n",
        "                        if self.file_map[file_id] in self.inv_index[term][1]:\n",
        "                            # Add document position to list\n",
        "                            self.inv_index[term][1][self.file_map[file_id]].append(position)\n",
        "                        # If this is first occurrance of the term in current document\n",
        "                        else:\n",
        "                            # Increment document freq by 1\n",
        "                            self.inv_index[term][0] += 1\n",
        "                            # Initialize list of positions in document with current position\n",
        "                            self.inv_index[term][1][self.file_map[file_id]] = [position]\n",
        "                    # If this is first occurrance of the term in inverted index\n",
        "                    else:\n",
        "                        # Initialize list with term as key\n",
        "                        # item 0 is 1 since this is the first occurrance\n",
        "                        # so document frequency is 1\n",
        "                        self.inv_index[term] = [1, {}]\n",
        "                        # Add current occurance\n",
        "                        self.inv_index[term][1][self.file_map[file_id]] = [position]\n",
        "\n",
        "            # for keeping track of cumulative offset: Without this, the postion ofupcoming multi-word tokens is not correct due to the length of the multi-word\n",
        "            # see report for detail\n",
        "            c_offset = 0\n",
        "            # Iterate through each tokens in list of tokens with multi-word tokens(token_tuple[1])\n",
        "            for position, term in enumerate(token_tuple[1]):\n",
        "                # Skip any occurrances of single-word tokens\n",
        "                if len(term.split(\" \")) > 1:\n",
        "                    # If term has occurred before\n",
        "                    if term in self.inv_index:\n",
        "                        # If term has occurred before in current document\n",
        "                        if self.file_map[file_id] in self.inv_index[term][1]:\n",
        "                            # Add document position to list\n",
        "                            self.inv_index[term][1][self.file_map[file_id]].append(position + c_offset)\n",
        "                        # If this is first occurrance of the term in current document\n",
        "                        else:\n",
        "                            # Increment document freq by 1\n",
        "                            self.inv_index[term][0] += 1\n",
        "                            # Initialize list of positions in document with current position\n",
        "                            self.inv_index[term][1][self.file_map[file_id]] = [position + c_offset]\n",
        "                    # If this is first occurrance of the term in inverted index\n",
        "                    else:\n",
        "                        # Initialize list with term as key\n",
        "                        # item 0 is 1 since this is the first occurrance\n",
        "                        # so document frequency is 1\n",
        "                        self.inv_index[term] = [1, {}]\n",
        "                        # Add current occurance\n",
        "                        self.inv_index[term][1][self.file_map[file_id]] = [position + c_offset]\n",
        "                    # increment cumulative offset\n",
        "                    c_offset += len(term.split(\" \")) - 1\n",
        "\n",
        "     \n",
        "    def dump(self, path: str) -> None:\n",
        "        \"\"\"\n",
        "        provide a dump function to show index entries for a given set of terms        \n",
        "        \"\"\"\n",
        "        # Open the file with development examples\n",
        "        with open(path, \"r\", encoding=\"utf8\") as file:\n",
        "            _text = file.read()\n",
        "        file.close()\n",
        "\n",
        "        _text = _text.lower().split(\"\\n\")\n",
        "        pos_tagged = nltk.pos_tag(_text)\n",
        "        print(pos_tagged)\n",
        "        wordnet_tagged = list(map(lambda x: (x[0], self.pos_tagger(x[1])), pos_tagged))\n",
        "        print(wordnet_tagged)\n",
        "        _text_lemmatized = [\n",
        "            self.wordnet_lemmatizer.lemmatize(word, tag) if len(word.split(\" \")) < 2 else word for word, tag in wordnet_tagged\n",
        "        ]\n",
        "\n",
        "        for index, term in enumerate(_text_lemmatized):\n",
        "            print(\"Input: \" + _text[index] + \", Lemmatized: \" + term)\n",
        "            if term in self.inv_index:\n",
        "                print(self.inv_index[term])\n",
        "            else:\n",
        "                print(\"Term not present in index\")\n",
        "\n",
        "       \n",
        "    def proximity_search(self, term1: str, term2: str, window_size: int) -> dict:\n",
        "        \"\"\"\n",
        "        This is Task 2\"\"\"\n",
        "        #returning search result\n",
        "        search_results = {}\n",
        "\n",
        "        # Setting a list of given terms for searching\n",
        "        search_terms = [term1, term2]\n",
        "        \n",
        "        # lemmmatize the search terms more details are given in the report\n",
        "        lem_search_terms = [self.wordnet_lemmatizer.lemmatize(search_terms[0]),\n",
        "                            self.wordnet_lemmatizer.lemmatize(search_terms[1])]\n",
        "\n",
        "        # setting indices in inverted index for term1 and term2\n",
        "        term1_indices = self.inv_index[lem_search_terms[0]]\n",
        "        term2_indices = self.inv_index[lem_search_terms[1]]\n",
        "\n",
        "        # setting window size\n",
        "        self.window_size = window_size\n",
        "\n",
        "        # Store and list the files that has both terms occuring\n",
        "        comm_docs = [\n",
        "            key\n",
        "            for key in term1_indices[1]\n",
        "            if key in term2_indices[1]\n",
        "        ]\n",
        "\n",
        "        for doc in comm_docs:\n",
        "          # setting indices of common document containing term1 and term2\n",
        "          t1_indices_in_doc = term1_indices[1][doc]\n",
        "          t2_indices_in_doc = term2_indices[1][doc]\n",
        "\n",
        "          # print(t1_indices_in_doc)\n",
        "          # print(t2_indices_in_doc)\n",
        "          for t1_index in t1_indices_in_doc:\n",
        "            for t2_index in t2_indices_in_doc:\n",
        "              # Get indices when absolute proximity of the two terms is smaller than the given window_size\n",
        "              if abs(t2_index - t1_index) <= self.window_size - 1:\n",
        "                if doc not in search_results:\n",
        "                  search_results[doc] = [1,[(t1_index, t2_index)]]\n",
        "                \n",
        "                else:\n",
        "                  search_results[doc][0] += 1\n",
        "                  search_results[doc][1].append((t1_index,t2_index))\n",
        "        \n",
        "\n",
        "        return search_results\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y-1t30g-0mF",
        "outputId": "00c02aa7-79ef-47b0-d243-e9de79244123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('bart', 'NN'), ('first', 'RB'), ('image', 'NN'), ('montage', 'NN'), ('well', 'RB'), ('top', 'JJ'), ('arguably', 'RB'), ('best', 'JJS'), ('number', 'NN'), ('humor', 'NN'), ('dollarydoos', 'JJ'), ('bart simpson', 'NN'), ('gordie howe', 'NN'), ('recalled', 'VBD'), ('bart the lover', 'NN'), ('cents', 'NNS'), ('won', 'VBD'), ('voice-overs', 'NNS'), ('simpsonovi', 'NN')]\n",
            "[('bart', 'n'), ('first', 'r'), ('image', 'n'), ('montage', 'n'), ('well', 'r'), ('top', 'a'), ('arguably', 'r'), ('best', 'a'), ('number', 'n'), ('humor', 'n'), ('dollarydoos', 'a'), ('bart simpson', 'n'), ('gordie howe', 'n'), ('recalled', 'v'), ('bart the lover', 'n'), ('cents', 'n'), ('won', 'v'), ('voice-overs', 'n'), ('simpsonovi', 'n')]\n",
            "Input: bart, Lemmatized: bart\n",
            "[114, {'3.1.txt': [201, 469, 519, 619, 656, 662, 687, 708, 959, 980, 1416, 1421, 1812, 1849, 2034, 2947, 3001], '3.10.txt': [1623], '3.11.txt': [683, 1173, 1222], '3.12.txt': [144, 209, 371, 451, 796, 800, 837, 944, 1129], '3.13.txt': [1, 14, 98, 133, 180, 201, 255, 344, 364, 408, 489, 516, 526, 547, 560, 603, 620, 642, 1081, 1218, 1252, 1352, 1413, 1534, 1591, 1616, 1891, 1907], '3.14.txt': [93, 623, 662], '3.15.txt': [91, 166, 218, 454, 497, 650, 866, 913, 935, 1172], '3.16.txt': [0, 13, 90, 127, 146, 206, 272, 372, 396, 419, 460, 488, 515, 539, 561, 577, 658, 783, 807, 820, 1015, 1050, 1202, 1351, 1513, 1633, 1734], '3.17.txt': [149, 1722], '3.18.txt': [62, 169, 219, 396, 445, 452, 477, 610, 627, 706, 1029, 1084, 1141, 1175, 1434, 1489, 1606, 1651], '3.19.txt': [214, 380, 425, 566, 599, 613, 1283, 2078], '3.2.txt': [478, 1354], '3.20.txt': [478, 2873], '3.21.txt': [205, 221, 329, 397, 402, 499, 556, 597, 623, 640, 954, 1079, 1326], '3.22.txt': [160, 372, 403, 430, 571, 591], '3.23.txt': [24, 198, 209, 367, 376, 436, 457, 475, 506, 514, 521, 541, 781, 809, 847, 849, 861, 875, 963], '3.24.txt': [458, 593], '3.3.txt': [100, 205, 527, 560, 623, 640, 1430, 1439, 1485], '3.4.txt': [0, 14, 72, 124, 161, 183, 197, 333, 377, 407, 433, 478, 498, 510, 532, 557, 568, 653, 672, 705, 1046, 1053, 1095, 1137, 1195, 1235, 1264, 1336, 1548, 1634], '3.5.txt': [68, 110, 206, 591, 613, 632, 658, 666, 675, 757, 833, 1363, 1404, 1435, 1463, 1480, 1484, 1496], '3.6.txt': [73, 180, 380, 397, 430, 588, 641, 711, 761, 1054, 1323, 1522, 1582], '3.7.txt': [141, 222, 476, 497, 528, 648, 656, 689, 700, 715, 727, 1446, 1704, 1790, 2078], '3.8.txt': [55, 553], '3.9.txt': [146, 157, 171, 192, 324, 336, 357, 394, 417, 432, 460, 486, 516, 592, 605, 751, 773, 1054, 1151, 1453, 1476], '4.1.txt': [192, 237, 247, 276, 381, 433, 453, 461, 474, 660, 1108], '4.10.txt': [27, 31, 356, 516, 546, 559, 578, 589, 595, 802, 825, 898, 910, 929, 1837, 1856, 1878, 1941, 1961, 1994, 2109], '4.11.txt': [613, 912, 1010], '4.12.txt': [629], '4.13.txt': [180, 234, 394, 459, 477, 501, 550, 896, 956], '4.14.txt': [163, 308, 332, 350, 440, 624, 691, 992, 1006, 1034, 1049, 1075, 1108, 1131, 1277, 1289, 1293], '4.15.txt': [595, 1202, 1335, 1518, 1617, 1708], '4.16.txt': [167, 297, 314, 371, 693, 726, 914, 941], '4.18.txt': [159, 164, 337, 347, 463, 493, 535, 803, 908, 1061, 1152, 1162, 1168, 1174, 1196, 1231, 1372], '4.19.txt': [143, 241, 319, 335, 346, 449, 453, 716, 982, 1033, 1269], '4.2.txt': [367, 1726, 2687], '4.20.txt': [167, 301, 322, 362, 368, 450], '4.21.txt': [588, 921], '4.22.txt': [220, 377, 561, 573, 607, 660, 2392], '4.4.txt': [78], '4.5.txt': [149, 163, 288, 323, 366, 734, 768, 789, 891, 1161, 1408], '4.6.txt': [133, 162, 225, 244, 267, 296, 301, 324, 337, 382, 397, 407, 452, 465, 484, 503, 537], '4.7.txt': [211, 421, 439, 449, 483, 580, 594, 600, 621, 819, 841, 1129, 1149, 1211, 1225, 1295, 1311], '4.8.txt': [196, 309, 341, 444, 490, 571], '4.9.txt': [1437], '5.1.txt': [258, 447, 1840, 2364], '5.10.txt': [223, 454], '5.11.txt': [101, 860, 1167, 1406], '5.12.txt': [0, 13, 19, 28, 124, 161, 187, 318, 368, 383, 398, 417, 425, 451, 492, 502, 523, 544, 595, 614, 634, 705, 731, 875, 939, 1031, 1174, 1200, 1214, 1243, 1263, 1288, 1375, 1419, 1453, 1502], '5.13.txt': [100, 893], '5.14.txt': [1445], '5.15.txt': [1102, 1269, 1297, 1348], '5.16.txt': [92, 915, 1565], '5.17.txt': [0, 14, 108, 146, 183, 337, 361, 375, 400, 427, 457, 479, 537, 626, 658, 717, 939, 1054, 1169, 1590, 1667], '5.18.txt': [74, 102, 178, 190, 211, 374, 390, 450, 456, 494, 502, 523, 537, 604, 624, 653, 729, 757, 1196], '5.19.txt': [187, 337, 479, 564, 575, 594, 600, 710, 730, 746, 805, 905, 1060, 1164, 1370, 1411], '5.2.txt': [572, 797, 1316], '5.20.txt': [181, 193, 358, 382, 436, 459, 469, 531, 591, 609, 634, 738, 741, 778, 831, 846, 924, 1005, 1013, 1032, 1091, 1230, 1520, 1600, 1619], '5.21.txt': [426, 454, 483, 526, 1197, 1230, 1469], '5.22.txt': [105, 538, 554, 740, 787], '5.3.txt': [636, 1643, 2596], '5.4.txt': [571], '5.5.txt': [82, 228, 359, 386, 789, 824, 877, 906, 922, 975, 994, 1024, 1028, 1035, 1159, 1206, 1242, 1271, 1287, 1300, 1518, 1914, 1989, 2082, 2100, 2365, 2548, 2596, 2752], '5.6.txt': [366], '5.7.txt': [185, 217, 404, 452, 511, 586, 620, 630, 717], '5.8.txt': [170, 187, 329, 375, 397, 449, 484, 583, 609, 854, 1002, 1054, 1389, 1606, 1638], '5.9.txt': [73, 220, 545, 596, 667, 681, 1263, 2388], '6.1.txt': [0, 13, 105, 142, 232, 292, 310, 358, 378, 397, 454, 497, 522, 688, 751, 759, 800, 855, 1168, 1334, 1443, 1635], '6.10.txt': [68, 499, 566, 570], '6.12.txt': [524], '6.13.txt': [558], '6.14.txt': [131, 222, 248], '6.15.txt': [119], '6.16.txt': [0, 13, 111, 148, 163, 195, 242, 270, 307, 324, 341, 368, 390, 395, 431, 444, 464, 477, 497, 531, 541, 560, 606, 612, 882, 1063, 1115, 1201, 1334, 1488, 1960], '6.17.txt': [98, 171, 232, 606, 635, 658, 681, 695, 848, 967, 1397], '6.18.txt': [1095, 1108, 1118, 1127], '6.19.txt': [314, 778, 841, 862, 904], '6.2.txt': [93, 452, 745, 788], '6.20.txt': [463, 503, 543, 581, 603, 1032], '6.21.txt': [156, 333, 393, 540, 589, 1089, 1254, 1295, 1303], '6.22.txt': [166, 187, 448, 467, 485, 507, 533, 677, 697, 757, 781, 808, 1323, 1597], '6.23.txt': [1190], '6.24.txt': [268, 353, 436, 518, 628, 746, 772, 1009], '6.25.txt': [636, 1643, 2596], '6.3.txt': [847, 943], '6.4.txt': [184, 208, 252, 260, 290, 339, 899, 1085], '6.5.txt': [179, 343, 372, 433, 460, 518, 583, 635], '6.6.txt': [420, 485, 630, 798, 1019, 1048, 1064, 1105, 1113, 1184], '6.7.txt': [159, 166, 204, 237, 330, 350, 371, 387, 420, 435, 484, 500, 513, 587, 595, 622, 678, 839, 871, 917, 1259, 1281, 1326, 1330, 1369, 1440, 1464, 1503, 1531, 1578, 1625, 1721, 1730], '6.8.txt': [152, 294, 310, 381, 408, 418, 439, 650, 839, 895, 1060, 1268], '6.9.txt': [207, 763], '7.1.txt': [636, 1643, 2596], '7.10.txt': [1497, 1501, 1539], '7.11.txt': [165, 180, 282, 316, 335, 372, 395, 436, 456, 476, 492, 517, 532, 552, 575, 589, 933, 969, 1108, 1131, 1159, 1270, 1376], '7.12.txt': [518, 1151, 1160], '7.13.txt': [47, 186, 320, 352, 369, 393, 415, 471, 501, 786, 804, 1490, 1535, 1567, 1789], '7.14.txt': [96], '7.15.txt': [0, 20, 124, 161, 190, 308, 319, 339, 362, 377, 455, 460, 491, 555, 1091, 1139, 1187, 1225, 1303, 1324], '7.16.txt': [84], '7.18.txt': [42, 244, 262, 329, 466, 476, 502, 539, 563, 591, 618, 636, 653, 708, 760, 846, 878, 995, 1157, 1208, 1411, 1976, 2233, 2288], '7.19.txt': [95], '7.2.txt': [167, 171, 347, 470, 496, 521, 542, 550, 606], '7.20.txt': [0, 35, 112, 150, 185, 380, 403, 443, 539, 574, 609, 630, 661, 679, 775, 869, 1133, 1278, 1399, 1426, 1466, 1660], '7.21.txt': [151, 431, 441, 478, 790], '7.22.txt': [273, 390, 587, 641, 663, 695, 736, 788, 803, 828, 1423], '7.23.txt': [94], '7.24.txt': [312, 429], '7.25.txt': [167, 171, 347, 470, 496, 521, 542, 550, 606], '7.3.txt': [66, 108, 297, 365, 384, 475, 799, 893, 1024, 1096], '7.4.txt': [0, 14, 21, 107, 145, 190, 342, 408, 421, 422, 465, 470, 483, 544, 557, 578, 594, 618, 625, 647, 735, 1106, 1205, 1256, 1353, 1532, 1578, 1583, 1597, 1714, 1731, 1750, 1777, 1790, 1938, 2003, 2040, 2259, 2277, 2293, 2306], '7.5.txt': [91, 421, 545], '7.6.txt': [772, 855, 920, 934, 952, 966, 994, 1093, 1212, 1223, 1235, 1864, 1883, 1910], '7.7.txt': [367, 1191, 1687], '7.8.txt': [420], '7.9.txt': [181, 499, 617, 661, 688]}]\n",
            "Input: first, Lemmatized: first\n",
            "[103, {'3.1.txt': [138, 153, 1518, 1553, 2491, 2897, 3145], '3.10.txt': [135, 228, 353, 983, 1224, 2098, 2415], '3.11.txt': [156, 420], '3.12.txt': [318, 338, 1328, 1334], '3.13.txt': [363, 735], '3.15.txt': [1033, 1157], '3.16.txt': [110, 213, 829], '3.17.txt': [356, 847, 1002, 1824, 2118], '3.18.txt': [543, 597, 1014], '3.19.txt': [71, 1501, 1567], '3.2.txt': [217, 717, 816, 1206, 2181], '3.20.txt': [877, 1327, 1393, 1534, 2715, 2830], '3.21.txt': [873, 893], '3.22.txt': [143, 242, 268, 719, 737, 778], '3.3.txt': [730, 817], '3.4.txt': [221, 379, 779, 1011, 1554, 1603], '3.5.txt': [266, 722, 774, 819, 856, 884, 898, 996, 1120], '3.6.txt': [218, 968], '3.7.txt': [108, 166, 330, 523, 968, 2031, 2114], '3.8.txt': [69, 255, 353, 906, 1072, 1310], '3.9.txt': [522], '4.1.txt': [102, 591, 788, 800, 1127], '4.10.txt': [1, 15, 23, 111, 131, 163, 173, 183, 275, 343, 576, 648, 651, 682, 936, 1289, 1545, 1689, 1715, 1776, 1949, 2050], '4.11.txt': [91], '4.12.txt': [309, 804, 834, 1248, 1474, 1985], '4.14.txt': [135, 1066, 1468], '4.15.txt': [746, 772], '4.16.txt': [178, 611, 678, 723], '4.17.txt': [1109, 1724], '4.18.txt': [228, 237, 687, 881, 888], '4.19.txt': [400, 973], '4.2.txt': [135, 876], '4.20.txt': [247, 792, 983], '4.21.txt': [629, 653, 850], '4.22.txt': [177, 1265, 1277], '4.3.txt': [810, 828, 841, 885, 1463], '4.4.txt': [74, 152, 529], '4.6.txt': [592, 676, 753], '4.8.txt': [314, 805], '4.9.txt': [116, 1783], '5.1.txt': [161, 561, 658, 905, 1724], '5.10.txt': [816, 1090, 1115], '5.12.txt': [215, 546, 788, 1036, 1135, 1517], '5.13.txt': [545, 728, 875, 916, 1708], '5.15.txt': [180, 463, 916, 924], '5.17.txt': [246], '5.18.txt': [681, 787], '5.19.txt': [919, 954, 1053, 1534], '5.2.txt': [1335, 1457], '5.21.txt': [1027], '5.22.txt': [391, 405], '5.3.txt': [2024, 2740, 3317, 3998], '5.4.txt': [118, 190, 872, 960], '5.5.txt': [575, 1340, 1629, 1643, 1882, 2180], '5.7.txt': [232], '5.8.txt': [1543], '5.9.txt': [565, 567, 1786], '6.1.txt': [110, 468, 1407], '6.10.txt': [132], '6.11.txt': [151], '6.12.txt': [879], '6.13.txt': [176, 618, 700], '6.14.txt': [605, 737], '6.15.txt': [745, 965], '6.16.txt': [1639], '6.17.txt': [732], '6.18.txt': [151, 370, 926, 1704], '6.19.txt': [853, 1355], '6.2.txt': [208, 763, 919], '6.20.txt': [839, 877], '6.21.txt': [63], '6.22.txt': [279, 342, 966, 1107, 1241, 1320], '6.23.txt': [410], '6.24.txt': [47, 1071], '6.25.txt': [2024, 2740, 3317, 3998], '6.3.txt': [665, 1263], '6.4.txt': [125], '6.5.txt': [275, 858, 1047], '6.6.txt': [277, 831, 1009, 1392, 1526, 1541, 1715, 1894, 2450], '6.7.txt': [820, 1071], '6.8.txt': [120, 562, 635], '6.9.txt': [960], '7.1.txt': [2024, 2740, 3317, 3998], '7.10.txt': [112, 837], '7.11.txt': [762, 771, 803], '7.12.txt': [787], '7.13.txt': [1248, 1273], '7.14.txt': [204, 546, 577, 704], '7.15.txt': [930], '7.16.txt': [215, 1199, 1808], '7.17.txt': [791], '7.18.txt': [1250, 1692, 1781, 1796, 1818, 2167], '7.19.txt': [739, 1418], '7.20.txt': [601, 944, 1224], '7.21.txt': [1090, 1105, 1438], '7.23.txt': [1048], '7.3.txt': [213, 507, 1004], '7.4.txt': [128, 896, 1067], '7.5.txt': [195, 790, 808, 946], '7.6.txt': [166, 313, 377, 831, 1332, 1410, 1619, 2011, 2096], '7.7.txt': [701, 780], '7.8.txt': [157, 206, 218], '7.9.txt': [240, 792, 872]}]\n",
            "Input: image, Lemmatized: image\n",
            "[15, {'3.16.txt': [308, 1086, 1120, 1862], '3.18.txt': [1335], '3.24.txt': [1485], '3.7.txt': [1493, 1943], '3.9.txt': [1269], '4.15.txt': [30], '5.12.txt': [26], '5.13.txt': [2117], '5.4.txt': [19], '7.18.txt': [25, 421], '7.2.txt': [365], '7.21.txt': [2547], '7.25.txt': [365], '7.5.txt': [2421], '7.6.txt': [359, 1813]}]\n",
            "Input: montage, Lemmatized: montage\n",
            "[7, {'3.16.txt': [881], '3.21.txt': [1224], '4.18.txt': [598, 664, 1203], '4.4.txt': [911], '6.3.txt': [770, 779, 931], '7.10.txt': [94, 458, 742, 1229, 1416, 1544, 1557, 1568, 1601], '7.3.txt': [696, 706]}]\n",
            "Input: well, Lemmatized: well\n",
            "[114, {'3.1.txt': [2106, 2413, 2863], '3.10.txt': [395, 1119, 1778, 2378], '3.11.txt': [1603], '3.12.txt': [1511], '3.13.txt': [159, 418, 441, 451, 486, 555, 635, 817, 857, 1115, 1179, 1222, 1714, 1750, 1807], '3.14.txt': [1587, 1608], '3.15.txt': [258, 703, 1144], '3.16.txt': [480, 652, 1524, 1605, 1688], '3.17.txt': [1350, 1911, 2464], '3.18.txt': [1514, 1578, 1715], '3.19.txt': [1880], '3.2.txt': [2092, 2325, 2382, 2411, 2890], '3.20.txt': [2470], '3.21.txt': [1169, 1449, 1557], '3.22.txt': [1012, 1265], '3.23.txt': [1259, 1359], '3.24.txt': [1659], '3.4.txt': [1436], '3.5.txt': [1326], '3.6.txt': [1485], '3.7.txt': [1801, 2012], '3.8.txt': [1482], '3.9.txt': [1700], '4.1.txt': [654, 1124, 1221], '4.10.txt': [1160, 1476], '4.11.txt': [1125], '4.12.txt': [865, 1131, 1517, 1942], '4.13.txt': [350, 527, 601, 1117, 1140], '4.14.txt': [1254], '4.15.txt': [306, 1479, 1811], '4.16.txt': [1020], '4.17.txt': [958], '4.18.txt': [391], '4.19.txt': [1582], '4.2.txt': [248, 316], '4.21.txt': [340, 1398, 1545, 1561, 1646], '4.22.txt': [1921], '4.3.txt': [849, 860, 1311], '4.4.txt': [267, 1039], '4.5.txt': [1502], '4.6.txt': [1119], '4.7.txt': [962], '4.8.txt': [953], '4.9.txt': [209, 470, 1644], '5.1.txt': [777], '5.10.txt': [870, 1576], '5.11.txt': [1215], '5.12.txt': [1349], '5.13.txt': [1486, 1612, 1717], '5.14.txt': [1018, 1493], '5.15.txt': [1516], '5.16.txt': [1296, 1331], '5.17.txt': [640, 1498], '5.18.txt': [1117], '5.19.txt': [352, 410, 551, 1451], '5.2.txt': [286, 511], '5.20.txt': [964, 1482, 1559], '5.21.txt': [1415, 1454], '5.22.txt': [1511, 1583], '5.3.txt': [669, 1023, 2873, 3815, 3873], '5.4.txt': [1039], '5.5.txt': [292, 2379, 2492, 2643], '5.6.txt': [1033], '5.7.txt': [567, 1404], '5.8.txt': [267, 845, 1288], '5.9.txt': [1200, 1530, 2327], '6.1.txt': [784, 1604], '6.10.txt': [253, 1023], '6.11.txt': [253, 1193, 1300], '6.12.txt': [245, 927], '6.13.txt': [1200, 1366], '6.14.txt': [859], '6.15.txt': [1175, 1203, 1229], '6.16.txt': [812, 1685, 1710], '6.17.txt': [386, 1297], '6.18.txt': [432, 935, 1175, 1412, 2129], '6.19.txt': [1270], '6.2.txt': [1310], '6.21.txt': [1027, 1208], '6.22.txt': [302, 1033, 1821], '6.23.txt': [252, 919, 1616], '6.24.txt': [994], '6.25.txt': [669, 1023, 2873, 3815, 3873], '6.4.txt': [248, 673], '6.5.txt': [201, 261, 938, 1154, 1484, 1590, 1683], '6.6.txt': [1284, 2442, 2564, 2638], '6.7.txt': [1433], '6.8.txt': [1042], '6.9.txt': [641], '7.1.txt': [669, 1023, 2873, 3815, 3873], '7.10.txt': [1185, 2219, 2364], '7.11.txt': [675, 1078, 1136], '7.12.txt': [1523], '7.13.txt': [1738, 1863, 1894], '7.14.txt': [644, 686, 932], '7.15.txt': [1212], '7.16.txt': [2102], '7.17.txt': [1284], '7.18.txt': [2189], '7.19.txt': [1064, 1204, 1721, 1746, 1850], '7.2.txt': [1208], '7.20.txt': [1350, 1477], '7.21.txt': [1044, 1994, 2301], '7.22.txt': [1347], '7.23.txt': [1563], '7.24.txt': [1108], '7.25.txt': [1208], '7.3.txt': [1077], '7.4.txt': [258, 1887, 2130], '7.5.txt': [176, 2514], '7.6.txt': [996, 1055, 1789, 2609], '7.7.txt': [1626, 1655, 1805, 1841], '7.8.txt': [683, 1687], '7.9.txt': [1392, 1458]}]\n",
            "Input: top, Lemmatized: top\n",
            "[51, {'3.1.txt': [2692], '3.10.txt': [2165, 2278], '3.16.txt': [1731, 1765, 1839, 1854], '3.17.txt': [2046, 2081, 2126, 2259, 2319], '3.23.txt': [1573], '3.4.txt': [95], '3.5.txt': [1532], '3.6.txt': [1469, 1507], '3.7.txt': [2126, 2187], '3.8.txt': [691, 1827], '4.1.txt': [1249], '4.12.txt': [1295, 1308, 1501], '4.15.txt': [336, 1559, 1574, 1595], '4.16.txt': [1077], '4.17.txt': [1483, 1509, 1709], '4.2.txt': [2040, 2057], '4.22.txt': [1823, 2014, 2047], '4.3.txt': [1477], '4.6.txt': [645], '4.8.txt': [932], '4.9.txt': [1538], '5.1.txt': [2514, 2691], '5.13.txt': [1105, 1936, 2068], '5.15.txt': [1705], '5.18.txt': [841], '5.22.txt': [1762], '5.3.txt': [3710], '5.4.txt': [1264], '5.9.txt': [2185, 2216, 2490], '6.1.txt': [65], '6.14.txt': [1085], '6.15.txt': [102], '6.16.txt': [1446], '6.19.txt': [1195, 1221, 1248, 1360], '6.2.txt': [280, 998, 1270], '6.20.txt': [1727], '6.21.txt': [1586], '6.23.txt': [858], '6.24.txt': [469], '6.25.txt': [3710], '6.4.txt': [961, 1032, 1052], '6.6.txt': [2254, 2652], '6.9.txt': [927], '7.1.txt': [3710], '7.18.txt': [453, 1660, 1944], '7.19.txt': [287, 1145, 1566], '7.21.txt': [999, 1946, 1960, 2044], '7.24.txt': [1079, 1146], '7.4.txt': [329, 2211, 2285], '7.6.txt': [2810], '7.7.txt': [1005, 1024, 1438, 1449, 1549, 1964]}]\n",
            "Input: arguably, Lemmatized: arguably\n",
            "[3, {'3.16.txt': [1882], '4.12.txt': [1320], '5.9.txt': [2506]}]\n",
            "Input: best, Lemmatized: best\n",
            "[98, {'3.1.txt': [211, 737, 2385, 2594, 2743, 2842], '3.10.txt': [404, 1792, 1871, 2067, 2077, 2287, 2338, 2358, 2389], '3.11.txt': [1686, 1710], '3.12.txt': [1235], '3.13.txt': [1511, 1580, 1620, 1661, 1694, 1813], '3.14.txt': [1827], '3.15.txt': [1201], '3.16.txt': [1496, 1879, 1883], '3.17.txt': [352, 938, 2184, 2222, 2239, 2272], '3.18.txt': [145, 287, 1532, 1723], '3.19.txt': [1971], '3.2.txt': [1062, 2021, 2110, 2150], '3.20.txt': [1462, 1482, 2571], '3.21.txt': [756, 1180], '3.22.txt': [348, 1451, 1470, 1536], '3.23.txt': [174, 1518], '3.24.txt': [1715, 1869, 1990], '3.3.txt': [1154, 1527], '3.4.txt': [1396, 1470], '3.5.txt': [628, 1208, 1550], '3.7.txt': [414, 2153, 2166, 2218], '3.8.txt': [603, 1403, 1517, 1651, 1715], '3.9.txt': [743, 1355], '4.10.txt': [1622, 1744], '4.11.txt': [717], '4.12.txt': [273, 1283, 1359, 1373, 1434, 1480, 1633, 1706, 1734, 1767, 1881], '4.14.txt': [269, 1380], '4.15.txt': [1424], '4.17.txt': [245, 1473, 1629, 1689, 1800], '4.2.txt': [1975, 1989, 2094, 2666], '4.20.txt': [1170, 1253], '4.22.txt': [1879, 1918, 2254, 2365], '4.3.txt': [328, 1566, 1592], '4.4.txt': [1061], '4.5.txt': [1556], '4.6.txt': [1187], '4.7.txt': [1022], '4.9.txt': [219, 479, 1622, 1689, 1821, 1850], '5.1.txt': [1779, 2486, 2710], '5.10.txt': [1552], '5.12.txt': [1372, 1379, 1466], '5.13.txt': [1627, 1759, 1789, 1815, 1833, 1911], '5.14.txt': [1149, 1511, 1521, 1624], '5.15.txt': [315, 1841], '5.16.txt': [1497], '5.17.txt': [51, 1403, 1418, 1435], '5.18.txt': [1252], '5.2.txt': [755], '5.21.txt': [1286], '5.22.txt': [1118, 1274], '5.3.txt': [3762, 3794, 3893], '5.4.txt': [316, 1296, 1310, 1375, 1465], '5.5.txt': [2513, 2579, 2633, 2653, 2661, 2756, 2762], '5.7.txt': [291, 1276], '5.8.txt': [1721], '5.9.txt': [326, 2203, 2476, 2508, 2525], '6.11.txt': [1370], '6.12.txt': [224, 677, 759, 910, 1013, 1109, 1124], '6.15.txt': [225, 1361], '6.16.txt': [1720], '6.18.txt': [782, 1389, 1525, 1577], '6.19.txt': [1143, 1266, 1301, 1320, 1342, 1388], '6.2.txt': [1255], '6.20.txt': [299, 1177, 1825, 1843, 1858], '6.21.txt': [1225], '6.22.txt': [711], '6.23.txt': [1706], '6.24.txt': [1058], '6.25.txt': [3762, 3794, 3893], '6.4.txt': [417], '6.5.txt': [221, 1727, 1747], '6.6.txt': [2189, 2317, 2336, 2353, 2379, 2428, 2456, 2483, 2499, 2545, 2588, 2622], '6.7.txt': [299, 1652, 1668], '6.8.txt': [368, 413, 1133], '6.9.txt': [952], '7.1.txt': [3762, 3794, 3893], '7.11.txt': [1260, 1312], '7.12.txt': [1110, 1358, 1410, 1455], '7.13.txt': [224, 1244, 1685, 1912, 1926], '7.14.txt': [1052], '7.15.txt': [697, 1178], '7.16.txt': [2154, 2172], '7.17.txt': [1246], '7.18.txt': [2019, 2062], '7.19.txt': [1606, 1623, 1648, 1703, 1758], '7.2.txt': [1365, 1386, 1467, 1492], '7.20.txt': [1580, 1632], '7.21.txt': [1911, 2017, 2030, 2082, 2113, 2479], '7.23.txt': [1514, 1918], '7.24.txt': [1127, 1189], '7.25.txt': [1365, 1386, 1467, 1492], '7.3.txt': [595, 605, 976, 1014], '7.4.txt': [317, 331, 2213], '7.5.txt': [1989, 2004, 2023, 2570, 2651], '7.6.txt': [2712, 2759, 2784], '7.7.txt': [258, 1469, 1494, 1509, 1520, 1593, 1616, 1733, 1857, 1914, 1942], '7.8.txt': [1709, 1726], '7.9.txt': [1436, 1473]}]\n",
            "Input: number, Lemmatized: number\n",
            "[49, {'3.1.txt': [1423, 3161], '3.11.txt': [1699], '3.16.txt': [1740, 1867], '3.17.txt': [2253], '3.18.txt': [1704], '3.19.txt': [804], '3.21.txt': [707, 1201, 1208], '3.5.txt': [1541], '3.7.txt': [2200], '3.9.txt': [801], '4.1.txt': [777], '4.11.txt': [669, 672], '4.12.txt': [1383, 1494], '4.15.txt': [1589], '4.16.txt': [1070], '4.17.txt': [1481], '4.2.txt': [1196, 2051, 2552], '4.21.txt': [792, 804, 833], '4.22.txt': [1452, 1872, 2269], '4.7.txt': [1017], '4.9.txt': [744], '5.1.txt': [1943, 2190, 2197], '5.10.txt': [1514], '5.11.txt': [1237], '5.13.txt': [1256, 1744, 1817, 1825, 1860, 1929], '5.14.txt': [777], '5.2.txt': [1273], '5.21.txt': [1398], '5.22.txt': [1758], '5.3.txt': [1933], '5.8.txt': [1051, 1069], '6.12.txt': [57, 177, 536, 679, 706, 864, 1093], '6.2.txt': [386], '6.20.txt': [235, 1052, 1735, 1853], '6.22.txt': [1937], '6.24.txt': [749], '6.25.txt': [1933], '6.4.txt': [1028, 1046], '6.6.txt': [1179, 1763], '7.1.txt': [1933], '7.10.txt': [687, 1703, 2243], '7.13.txt': [1254], '7.14.txt': [837], '7.19.txt': [785], '7.21.txt': [2038], '7.23.txt': [922], '7.6.txt': [2020, 2159], '7.7.txt': [1958], '7.8.txt': [1134, 1191]}]\n",
            "Input: humor, Lemmatized: humor\n",
            "[23, {'3.1.txt': [2395, 2548], '3.10.txt': [2226], '3.13.txt': [1555], '3.15.txt': [764], '3.16.txt': [1579], '3.20.txt': [2535], '3.3.txt': [1244, 1425], '3.7.txt': [2239], '4.19.txt': [415], '5.1.txt': [407, 2564], '5.10.txt': [1056], '5.11.txt': [1303], '5.12.txt': [240, 720], '6.10.txt': [1201], '6.14.txt': [504, 650], '6.16.txt': [737], '6.20.txt': [66], '6.6.txt': [2277], '7.10.txt': [810], '7.14.txt': [667], '7.18.txt': [1057], '7.4.txt': [502], '7.5.txt': [2229]}]\n",
            "Input: dollarydoos, Lemmatized: dollarydoos\n",
            "[1, {'6.16.txt': [1915, 1931]}]\n",
            "Input: bart simpson, Lemmatized: bart simpson\n",
            "[16, {'3.13.txt': [516], '3.4.txt': [72, 1053], '4.2.txt': [2687], '4.7.txt': [1211], '5.1.txt': [258], '5.15.txt': [1102], '5.3.txt': [1643], '6.14.txt': [131], '6.18.txt': [1118], '6.21.txt': [156], '6.23.txt': [1190], '6.25.txt': [1643], '6.5.txt': [343], '6.7.txt': [917], '7.1.txt': [1643], '7.4.txt': [1790]}]\n",
            "Input: gordie howe, Lemmatized: gordie howe\n",
            "Term not present in index\n",
            "Input: recalled, Lemmatized: recall\n",
            "[21, {'3.1.txt': [1148], '3.11.txt': [1377], '3.14.txt': [927], '3.16.txt': [944], '3.17.txt': [2423], '3.19.txt': [591], '3.7.txt': [1621], '5.1.txt': [1483], '5.16.txt': [1386], '5.19.txt': [1518], '5.8.txt': [977, 1134], '6.11.txt': [1294], '6.2.txt': [954], '6.3.txt': [190, 888, 899], '6.4.txt': [929], '7.18.txt': [1327], '7.2.txt': [1069], '7.21.txt': [2268], '7.22.txt': [1218], '7.25.txt': [1069], '7.4.txt': [1014]}]\n",
            "Input: bart the lover, Lemmatized: bart the lover\n",
            "Term not present in index\n",
            "Input: cents, Lemmatized: cent\n",
            "[7, {'3.11.txt': [428], '3.16.txt': [747], '3.2.txt': [1315], '4.15.txt': [1155], '5.1.txt': [2149], '5.6.txt': [691], '7.4.txt': [838]}]\n",
            "Input: won, Lemmatized: win\n",
            "[45, {'3.13.txt': [1446, 1476], '3.14.txt': [152, 240, 362, 523, 535, 566, 590, 1016], '3.16.txt': [1411], '3.17.txt': [229, 427, 502, 615, 860, 1790, 1807], '3.18.txt': [1427, 1481], '3.19.txt': [346, 989, 1007, 1037, 1083, 1900], '3.2.txt': [141, 600], '3.21.txt': [747, 777, 1173], '3.24.txt': [487], '3.4.txt': [389], '3.6.txt': [338, 1341, 1392, 1420, 1437, 1442], '3.8.txt': [182, 1333], '3.9.txt': [373, 501, 1132], '4.10.txt': [457, 463, 485, 1380, 1399], '4.15.txt': [1145], '4.16.txt': [373], '4.17.txt': [386], '4.19.txt': [489, 526], '4.2.txt': [153, 2194, 2272], '4.20.txt': [265], '4.22.txt': [1934], '4.3.txt': [302], '4.4.txt': [328, 502, 849], '4.6.txt': [470, 478], '4.9.txt': [226, 1864, 1890, 1997], '5.1.txt': [83, 680], '5.10.txt': [447, 680], '5.15.txt': [354, 382], '5.16.txt': [292, 303, 336], '5.17.txt': [147, 338, 948, 1032, 1396], '5.22.txt': [614], '5.8.txt': [1711, 1717], '5.9.txt': [742], '6.19.txt': [185, 203, 1150, 1168], '6.2.txt': [343, 463], '6.22.txt': [1801], '6.5.txt': [167, 416], '6.8.txt': [1243], '7.12.txt': [444, 1025, 1100, 1105], '7.19.txt': [374], '7.2.txt': [493], '7.25.txt': [493], '7.5.txt': [330, 1982, 2315], '7.6.txt': [2882, 2928], '7.8.txt': [1621, 1648]}]\n",
            "Input: voice-overs, Lemmatized: voice-overs\n",
            "[1, {'3.16.txt': [1620]}]\n",
            "Input: simpsonovi, Lemmatized: simpsonovi\n",
            "Term not present in index\n",
            "13088\n",
            "EXECUTION TIME: 13.552028894424438 sec\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"main call function\"\n",
        "    start = time.time() # Checking the time before initializing the inverted index\n",
        "\n",
        "    index = InvertedIndex() # initilaise the index\n",
        "    corpus = index.read_data('drive/MyDrive/Simpsons-2022') # specify the directory path in which files are located\n",
        "    index.index_corpus(corpus) # index documents/corpus\n",
        "\n",
        "    end = time.time() # Checking the time after inverted index is created\n",
        "\n",
        "    # ## testing the dump function sith the development-examples.txt\n",
        "    index.dump('drive/MyDrive/ColabNotebooks/development-examples.txt')\n",
        "\n",
        "    # ## checking inverted index\n",
        "    # print(index.inv_index)\n",
        "\n",
        "\n",
        "    # ## print multi-words of inverted index\n",
        "    # print(\"\\n\")\n",
        "    # has_space = False\n",
        "    # for key in index.inv_index.keys():\n",
        "    #   for c in key:\n",
        "    #     if c.isspace():\n",
        "    #       has_space = True\n",
        "    #     else:\n",
        "    #       continue\n",
        "    #   if has_space == True:\n",
        "    #     print(key)\n",
        "    #     has_space = False\n",
        "    #   else:\n",
        "    #     continue\n",
        "\n",
        "    # ## checking if the keys of inverted index have $%#\n",
        "    # for key in index.inv_index:\n",
        "    #     if('$' in key or '%' in key or '#' in key):\n",
        "    #         print(key, index.inv_index[key])\n",
        "\n",
        "    # ## printing length of the inverted index(number of vocabs)\n",
        "    print(len(index.inv_index))\n",
        "\n",
        "    # ## testing proximity search\n",
        "    # print(index.proximity_search('rock','stars',3))\n",
        "    # print(index.proximity_search('rock','station',3))\n",
        "\n",
        "    # ## printing time taken to construct inverted index\n",
        "    print(\"EXECUTION TIME: {}\".format(end - start) + \" sec\")\n",
        "    return index\n",
        "index = main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}